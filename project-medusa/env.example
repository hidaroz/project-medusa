# ============================================================================
# MEDUSA Project - Environment Configuration Template
# ============================================================================
# Copy this file to .env and fill in your actual values
# Command: cp env.example .env
# ============================================================================

# ============================================================================
# LLM PROVIDER CONFIGURATION
# ============================================================================
# Provider Selection: "local" (default), "openai", "anthropic", "mock", or "auto"
# - local: Uses local Ollama instance (recommended, free, private)
# - openai: Uses OpenAI API (requires API key and payment)
# - anthropic: Uses Anthropic Claude API (requires API key and payment)
# - mock: Uses mock responses (testing only)
# - auto: Auto-detects best available (tries local first, then cloud)
LLM_PROVIDER=auto

# ============================================================================
# LOCAL PROVIDER CONFIGURATION (Default)
# ============================================================================
# Ollama server URL (must be running locally)
# Install Ollama: https://ollama.com/download
# Start server: ollama serve
# Pull model: ollama pull mistral:7b-instruct
OLLAMA_URL=http://localhost:11434

# Local model to use
# Default: mistral:7b-instruct (fast, free, recommended)
# Other options: llama2, neural-chat, dolphin-mixtral, etc.
LOCAL_MODEL=mistral:7b-instruct

# ============================================================================
# CLOUD PROVIDER CONFIGURATION (Optional)
# ============================================================================
# Cloud API Key (for OpenAI or Anthropic)
# Leave empty to use local provider
CLOUD_API_KEY=

# Cloud Model Selection
# For OpenAI: gpt-4-turbo-preview, gpt-4, gpt-3.5-turbo
# For Anthropic: claude-3-sonnet-20240229, claude-3-haiku-20240307
# CLOUD_MODEL=gpt-4-turbo-preview

# Cloud Provider Base URL (optional, for Azure OpenAI or custom endpoints)
# CLOUD_BASE_URL=https://api.openai.com/v1

# ============================================================================
# LLM GENERATION PARAMETERS
# ============================================================================
# Temperature (0.0 - 1.0): Lower = more focused, Higher = more creative
LLM_TEMPERATURE=0.7

# Maximum tokens for LLM responses
LLM_MAX_TOKENS=2048

# Request timeout (seconds)
LLM_TIMEOUT=60

# Maximum retry attempts on failure
LLM_RETRIES=3

# ============================================================================
# MEDUSA APPLICATION SETTINGS
# ============================================================================
# Application environment (development, production, lab)
APP_ENV=development

# Enable debug mode (true/false)
APP_DEBUG=true

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# MEDUSA home directory (stores logs, reports, config)
MEDUSA_HOME=~/.medusa

# ============================================================================
# NEO4J GRAPH DATABASE CONFIGURATION
# ============================================================================
# Neo4j connection URI (use bolt://localhost:7687 for local, bolt://medusa-neo4j:7687 for Docker)
NEO4J_URI=bolt://localhost:7687

# Neo4j credentials
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=medusa_graph_pass

# Neo4j database name
NEO4J_DATABASE=neo4j

# Neo4j logging
NEO4J_LOG_LEVEL=INFO
NEO4J_LOG_QUERIES=false

# Docker internal URI (auto-detected when running in Docker)
NEO4J_DOCKER_URI=bolt://medusa-neo4j:7687

# ============================================================================
# GRAPH API SERVICE CONFIGURATION
# ============================================================================
# Graph API port
GRAPH_API_PORT=5002

# Graph API host (0.0.0.0 for all interfaces, 127.0.0.1 for localhost only)
GRAPH_API_HOST=0.0.0.0

# Enable API authentication (true/false)
GRAPH_API_ENABLE_AUTH=true

# API key for authentication (CHANGE THIS IN PRODUCTION!)
GRAPH_API_KEY=medusa-dev-key-change-in-production

# Rate limiting (requests per window)
GRAPH_API_RATE_LIMIT=100
GRAPH_API_RATE_WINDOW=60

# Query safety limits
GRAPH_API_MAX_QUERY_LENGTH=10000
GRAPH_API_QUERY_TIMEOUT=30

# Flask environment (development, production)
FLASK_ENV=development

# ============================================================================
# DOCKER LAB ENVIRONMENT
# ============================================================================
# MySQL Database
MYSQL_ROOT_PASSWORD=admin123
MYSQL_DATABASE=healthcare_db
MYSQL_USER=ehrapp
MYSQL_PASSWORD=Welcome123!

# Database Host (use 'localhost' for external access, 'ehr-database' for internal)
DB_HOST=localhost
DB_PORT=3306

# ============================================================================
# NETWORK CONFIGURATION
# ============================================================================
# Docker network subnets
DMZ_SUBNET=172.20.0.0/24
INTERNAL_SUBNET=172.21.0.0/24

# Service ports (should match docker-compose.yml)
EHR_WEB_PORT=8080
EHR_API_PORT=3000
SSH_PORT=2222
FTP_PORT=21
LDAP_PORT=389
LOG_COLLECTOR_PORT=8081
MYSQL_PORT=3306
SMB_PORT=445

# ============================================================================
# SECURITY SETTINGS
# ============================================================================
# Approval gate for high-risk commands (true/false)
REQUIRE_APPROVAL=true

# Maximum risk level without approval (LOW, MEDIUM, HIGH, CRITICAL)
MAX_AUTO_RISK=MEDIUM

# Enable audit logging (true/false)
AUDIT_LOGGING=true

# Audit log file
AUDIT_LOG_FILE=~/.medusa/logs/audit.log

# ============================================================================
# REPORTING CONFIGURATION
# ============================================================================
# Default report format (text, json, html, markdown)
REPORT_FORMAT=html

# Report output directory
REPORT_OUTPUT_DIR=~/.medusa/reports

# Include screenshots in reports (true/false)
REPORT_SCREENSHOTS=true

# Include command history in reports (true/false)
REPORT_COMMAND_HISTORY=true

# ============================================================================
# ADVANCED SETTINGS
# ============================================================================
# Maximum concurrent operations
MAX_CONCURRENT_OPS=3

# Command timeout (seconds)
COMMAND_TIMEOUT=60

# Retry failed operations (true/false)
RETRY_ENABLED=true

# Maximum retry attempts
MAX_RETRIES=3

# ============================================================================
# OPTIONAL: RAG (Retrieval-Augmented Generation) SETTINGS
# ============================================================================
# Enable RAG for enhanced context (true/false)
RAG_ENABLED=false

# RAG vector database path
RAG_DB_PATH=~/.medusa/vectordb

# RAG embedding model
RAG_EMBEDDING_MODEL=text-embedding-004

# ============================================================================
# OPTIONAL: WEBHOOK/NOTIFICATION SETTINGS
# ============================================================================
# Enable notifications (true/false)
NOTIFICATIONS_ENABLED=false

# Slack webhook URL (optional)
SLACK_WEBHOOK_URL=

# Discord webhook URL (optional)
DISCORD_WEBHOOK_URL=

# Email notifications (optional)
EMAIL_ENABLED=false
SMTP_HOST=
SMTP_PORT=587
SMTP_USER=
SMTP_PASSWORD=
SMTP_FROM=

# ============================================================================
# OPTIONAL: INTEGRATION SETTINGS
# ============================================================================
# Enable integration with external tools (true/false)
INTEGRATIONS_ENABLED=false

# Metasploit RPC settings (if using)
MSF_RPC_HOST=localhost
MSF_RPC_PORT=55553
MSF_RPC_USER=msf
MSF_RPC_PASS=

# Nmap path (optional, defaults to system nmap)
NMAP_PATH=/usr/bin/nmap

# ============================================================================
# DEVELOPER SETTINGS
# ============================================================================
# Enable development mode features (true/false)
DEV_MODE=true

# Enable verbose logging (true/false)
VERBOSE=false

# Enable profiling (true/false)
PROFILING=false

# Enable mock LLM responses for testing (true/false)
MOCK_LLM=false

# ============================================================================
# NOTES
# ============================================================================
# - Lines starting with # are comments
# - Values with spaces should NOT be quoted
# - Use 'true' or 'false' for boolean values (lowercase)
# - Paths with ~ will expand to home directory
# - Required for LOCAL provider (default): Ollama installation
# - Required for CLOUD providers: CLOUD_API_KEY environment variable
# - To use local LLM (recommended):
#   1. Install Ollama: https://ollama.com/download
#   2. Run: ollama pull mistral:7b-instruct
#   3. Run: ollama serve
#   4. MEDUSA will auto-detect and use it
# - After editing, run: source .env (bash) or set -a; source .env; set +a
# ============================================================================

