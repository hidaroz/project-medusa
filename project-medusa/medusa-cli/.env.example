# ============================================================================
# MEDUSA CLI - Environment Variables Configuration
# ============================================================================
# Copy this file to .env and customize for your environment
# cp .env.example .env
#
# For security: Never commit .env file to version control!
# ============================================================================

# ============================================================================
# LLM Provider Configuration
# ============================================================================

# LLM Provider Selection
# Options: local, bedrock, openai, anthropic, auto
# - local: Ollama (free, private, recommended for development)
# - bedrock: AWS Bedrock (Claude 3.5 models, pay-per-use)
# - openai: OpenAI API (GPT models)
# - anthropic: Anthropic API (Claude models)
# - auto: Auto-detect best available provider
LLM_PROVIDER=auto

# ============================================================================
# AWS Bedrock Configuration (Required if LLM_PROVIDER=bedrock)
# ============================================================================

# AWS Credentials
# Get these from AWS IAM Console or via 'aws configure'
# AWS_ACCESS_KEY_ID=AKIA...
# AWS_SECRET_ACCESS_KEY=...

# AWS Region (Bedrock available in: us-east-1, us-west-2, eu-west-1, ap-southeast-1)
# AWS_REGION=us-west-2

# Model Selection for Smart Routing
# Smart Model: Used for complex reasoning, planning, reporting
# SMART_MODEL=anthropic.claude-3-5-sonnet-20241022-v2:0

# Fast Model: Used for tool execution, parsing, data extraction
# FAST_MODEL=anthropic.claude-3-5-haiku-20241022-v1:0

# Default Model (if not using smart routing)
# CLOUD_MODEL=anthropic.claude-3-5-haiku-20241022-v1:0

# ============================================================================
# Cost Tracking (Bedrock)
# ============================================================================
# Cost tracking is enabled by default for Bedrock
# Current pricing (per 1M tokens):
#   Sonnet: $3 input / $15 output
#   Haiku:  $0.80 input / $4 output
# Smart routing saves ~60% by using Haiku for simple tasks

# ============================================================================
# Local Provider (Ollama) Configuration
# ============================================================================

# Ollama API URL
# OLLAMA_URL=http://localhost:11434

# Local Model Name
# LOCAL_MODEL=mistral:7b-instruct
# Other options: llama3:8b, codellama:7b, mixtral:8x7b

# ============================================================================
# Cloud Providers (OpenAI/Anthropic) Configuration
# ============================================================================

# OpenAI/Anthropic API Key (if using provider=openai or provider=anthropic)
# CLOUD_API_KEY=sk-...

# API Base URL (optional, for custom endpoints)
# CLOUD_BASE_URL=https://api.openai.com/v1

# ============================================================================
# LLM Generation Parameters
# ============================================================================

# Temperature (0.0-1.0, higher = more creative)
# LLM_TEMPERATURE=0.7

# Max Tokens (maximum response length)
# LLM_MAX_TOKENS=2048

# Timeout (seconds)
# LLM_TIMEOUT=60

# ============================================================================
# Neo4j Graph Database (World Model)
# ============================================================================

# Neo4j Connection String
NEO4J_URI=bolt://localhost:7687

# Neo4j Credentials
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# ============================================================================
# MEDUSA Operation Configuration
# ============================================================================

# Risk Tolerance Level
# Options: low, medium, high
# Controls which actions require approval
RISK_TOLERANCE=medium

# Auto-Approve Level
# Options: none, low, medium, high
# Automatically approve actions up to this risk level
AUTO_APPROVE_LEVEL=low

# ============================================================================
# Target Environment
# ============================================================================

# Target Type: docker, custom
# TARGET_TYPE=docker

# Docker Network (if using Docker lab environment)
# DOCKER_NETWORK=medusa-lab

# ============================================================================
# Logging
# ============================================================================

# Log Level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# LOG_LEVEL=INFO

# Log File Path
# LOG_FILE=~/.medusa/logs/medusa.log

# ============================================================================
# Advanced Configuration
# ============================================================================

# Config File Path (default: ~/.medusa/config.yaml)
# MEDUSA_CONFIG_PATH=~/.medusa/config.yaml

# Enable Mock Mode (for testing only)
# MOCK_MODE=false

# ============================================================================
# AWS Bedrock Quick Setup Guide
# ============================================================================
#
# 1. Create AWS Account and enable Bedrock access
#
# 2. Request model access in AWS Console:
#    - Go to AWS Bedrock â†’ Model access
#    - Enable: Anthropic Claude 3.5 Sonnet
#    - Enable: Anthropic Claude 3.5 Haiku
#
# 3. Configure credentials (choose one method):
#
#    Method A (Recommended): AWS CLI
#      aws configure
#      # Enter your access key, secret key, and region
#
#    Method B: Environment Variables
#      export AWS_ACCESS_KEY_ID=AKIA...
#      export AWS_SECRET_ACCESS_KEY=...
#      export AWS_REGION=us-west-2
#
#    Method C: This .env file
#      Uncomment and set AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION above
#
# 4. Set provider:
#    export LLM_PROVIDER=bedrock
#
# 5. Verify setup:
#    medusa llm verify
#
# For detailed setup guide, see: docs/00-getting-started/bedrock-setup.md
# ============================================================================
