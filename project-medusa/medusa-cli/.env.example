# ============================================================================
# MEDUSA CLI Environment Variables
# ============================================================================
# Configuration template for MEDUSA penetration testing framework
# Copy to .env and fill in your values
# ============================================================================

# ============================================================================
# LLM PROVIDER CONFIGURATION
# ============================================================================
# Choose your LLM provider: local, bedrock, openai, anthropic, auto, mock
# - local: Use local Ollama instance (recommended - free, private)
# - bedrock: AWS Bedrock (Claude 3.5 Sonnet/Haiku)
# - openai: OpenAI GPT-4
# - anthropic: Anthropic Claude
# - auto: Auto-detect provider (tries local first, falls back to cloud)
# - mock: Mock LLM (for testing)
LLM_PROVIDER=auto

# ============================================================================
# LOCAL PROVIDER (Ollama)
# ============================================================================
# For local/offline LLM inference using Ollama
# Install Ollama: https://ollama.com

# Ollama server URL
OLLAMA_URL=http://localhost:11434

# Model to use with Ollama (examples: mistral:7b-instruct, llama2:latest)
LOCAL_MODEL=mistral:7b-instruct

# ============================================================================
# AWS BEDROCK CONFIGURATION
# ============================================================================
# For AWS Bedrock Claude models (Claude 3.5 Sonnet/Haiku)
# 
# Prerequisites:
# 1. AWS Account with Bedrock access enabled
# 2. Model access requested in AWS Bedrock console
# 3. IAM user with bedrock:InvokeModel permission
#
# Setup: https://docs.aws.amazon.com/bedrock/latest/userguide/setting-up.html

# AWS Credentials (use AWS CLI credential chain for security)
# Option 1: Set via environment variables (NOT RECOMMENDED - security risk)
# AWS_ACCESS_KEY_ID=your_access_key_here
# AWS_SECRET_ACCESS_KEY=your_secret_key_here

# Option 2: Use ~/.aws/credentials file (RECOMMENDED)
# Run: aws configure
# Or use IAM roles if running on AWS

# AWS Region where Bedrock is available
# Available regions: us-east-1, us-west-2, eu-west-1
AWS_REGION=us-west-2

# Bedrock Model IDs (optional - uses defaults if not specified)
# Sonnet (smart, slower, more expensive)
SMART_MODEL=anthropic.claude-3-5-sonnet-20241022-v2:0

# Haiku (fast, cheaper)
FAST_MODEL=anthropic.claude-3-5-haiku-20241022-v1:0

# General purpose model
CLOUD_MODEL=anthropic.claude-3-5-haiku-20241022-v1:0

# ============================================================================
# OPENAI CONFIGURATION
# ============================================================================
# For OpenAI GPT-4 or GPT-3.5-turbo

# OpenAI API key (get from https://platform.openai.com/api-keys)
# OPENAI_API_KEY=sk-...

# Model to use (gpt-4, gpt-3.5-turbo, etc.)
# OPENAI_MODEL=gpt-4

# ============================================================================
# ANTHROPIC CONFIGURATION
# ============================================================================
# For Anthropic Claude models (alternative to Bedrock)

# Anthropic API key (get from https://console.anthropic.com/)
# ANTHROPIC_API_KEY=sk-ant-...

# Model to use (claude-3-opus, claude-3-sonnet, etc.)
# ANTHROPIC_MODEL=claude-3-sonnet-20240229

# ============================================================================
# LLM GENERATION PARAMETERS
# ============================================================================
# These settings apply to all providers

# Temperature: Controls randomness (0.0 = deterministic, 1.0 = very random)
# For pentesting: 0.7 recommended (balanced between determinism and creativity)
LLM_TEMPERATURE=0.7

# Max tokens: Maximum length of generated response
LLM_MAX_TOKENS=2048

# Timeout: Seconds to wait for LLM response
LLM_TIMEOUT=60

# ============================================================================
# NEO4J GRAPH DATABASE (World Model)
# ============================================================================
# For persisting discovered network topology and attack relationships

# Neo4j connection URI
NEO4J_URI=bolt://localhost:7687

# Neo4j credentials
NEO4J_USER=neo4j

# Default password for neo4j (change after first login)
NEO4J_PASSWORD=password

# ============================================================================
# MEDUSA OPERATIONAL CONFIGURATION
# ============================================================================

# Risk Tolerance: Limits what operations are allowed
# - low: Only read-only scanning
# - medium: Reconnaissance + enumeration (no exploitation)
# - high: Full exploitation with approval gates
RISK_TOLERANCE=medium

# Auto-Approve Level: Operations automatically approved without prompting
# - none: All operations require approval
# - low: LOW risk operations auto-approved
# - medium: LOW + MEDIUM risk auto-approved
# - high: All except CRITICAL auto-approved
AUTO_APPROVE_LEVEL=low

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Log file location (relative or absolute path)
LOG_FILE=logs/medusa.log

# ============================================================================
# TARGET CONFIGURATION (Optional - can be set via CLI)
# ============================================================================

# Default target for scans
# TARGET_HOST=192.168.1.100
# TARGET_PORTS=80,443,22

# ============================================================================
# ADVANCED CONFIGURATION (Rarely needed)
# ============================================================================

# Mock mode: Use mock LLM for testing (ignores actual LLM provider)
# MOCK_MODE=false

# Max retries for LLM calls
LLM_MAX_RETRIES=3

# Cost tracking for Bedrock usage (AWS only)
# ENABLE_COST_TRACKING=true

# ============================================================================
# NOTES
# ============================================================================
# 1. Credentials in .env are local - never commit .env to git
# 2. For AWS: Use ~/.aws/credentials instead of env vars (more secure)
# 3. For Ollama: Make sure it's running before starting MEDUSA
# 4. For cloud providers: Be aware of rate limits and costs
# 5. See docs/00-getting-started/bedrock-setup.md for AWS setup guide
