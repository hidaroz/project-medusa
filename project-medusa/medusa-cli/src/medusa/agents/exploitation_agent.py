"""
Exploitation Agent
Specializes in exploit execution and attack chain implementation
"""

from typing import Dict, Any, List, Optional
import json
import asyncio
from enum import Enum

from .base_agent import BaseAgent, AgentCapability
from .data_models import AgentTask, AgentResult, AgentStatus


class ApprovalStatus(Enum):
    """Approval status for exploitation actions"""
    PENDING = "pending"
    APPROVED = "approved"
    REJECTED = "rejected"
    AUTO_APPROVED = "auto_approved"


class ExploitationAgent(BaseAgent):
    """
    Exploitation Agent

    Responsibilities:
    - Execute approved exploitation attempts
    - Manage approval gates for dangerous operations
    - Execute attack chains safely
    - Monitor exploitation success/failure
    - Provide post-exploitation recommendations

    This agent ONLY simulates exploits - it does not execute real attacks.
    All "execution" is analysis and recommendation-based.
    """

    def __init__(self, require_approval: bool = True, *args, **kwargs):
        """
        Initialize Exploitation Agent

        Args:
            require_approval: Whether to require approval for exploits
            *args, **kwargs: BaseAgent arguments
        """
        super().__init__(
            name="ExploitationAgent",
            capabilities=[AgentCapability.EXPLOITATION],
            *args,
            **kwargs
        )

        self.require_approval = require_approval
        self.pending_approvals: Dict[str, Dict[str, Any]] = {}
        self.executed_exploits: Dict[str, Dict[str, Any]] = {}

    async def execute_task(self, task: AgentTask) -> AgentResult:
        """
        Execute exploitation task

        Task types:
        - plan_exploitation: Plan exploitation approach
        - execute_exploit: Execute specific exploit (simulated)
        - verify_access: Verify successful exploitation
        - recommend_post_exploitation: Suggest post-exploit actions

        Args:
            task: Exploitation task

        Returns:
            AgentResult with exploitation results
        """
        self.logger.info(f"Executing exploitation task: {task.task_type}")

        if task.task_type == "plan_exploitation":
            return await self._plan_exploitation(task)
        elif task.task_type == "execute_exploit":
            return await self._execute_exploit(task)
        elif task.task_type == "verify_access":
            return await self._verify_access(task)
        elif task.task_type == "recommend_post_exploitation":
            return await self._recommend_post_exploitation(task)
        else:
            return AgentResult(
                task_id=task.task_id,
                agent_name=self.name,
                status=AgentStatus.FAILED,
                error=f"Unknown task type: {task.task_type}"
            )

    async def _plan_exploitation(self, task: AgentTask) -> AgentResult:
        """
        Plan exploitation approach based on vulnerabilities

        Uses context fusion to:
        - Find similar past exploitations
        - Get exploit frameworks and tools
        - Access MITRE ATT&CK exploitation techniques
        """
        vulnerabilities = task.parameters.get("vulnerabilities", [])
        target = task.parameters.get("target")
        constraints = task.parameters.get("constraints", {})

        # Build rich context
        context = {}
        if self.context_engine:
            try:
                context = self.context_engine.build_context_for_exploitation(
                    vulnerabilities=vulnerabilities,
                    target=target
                )
            except Exception as e:
                self.logger.warning(f"Failed to build context: {e}")

        # Build exploitation planning prompt
        prompt = self._build_exploitation_plan_prompt(vulnerabilities, target, constraints, context)

        # Use LLM with routing (MODERATE complexity)
        llm_response = await self.llm_client.generate_with_routing(
            prompt=prompt,
            task_type="plan_exploitation",
            force_json=True,
            max_tokens=3072
        )

        # Parse response
        try:
            plan = json.loads(llm_response.content)
        except json.JSONDecodeError:
            import re
            json_match = re.search(r'\{.*\}', llm_response.content, re.DOTALL)
            if json_match:
                plan = json.loads(json_match.group(0))
            else:
                plan = {"exploitation_steps": []}

        result = AgentResult(
            task_id=task.task_id,
            agent_name=self.name,
            status=AgentStatus.COMPLETED,
            findings=[plan],
            recommendations=plan.get("exploitation_steps", []),
            metadata={
                "target": target,
                "vulnerabilities_analyzed": len(vulnerabilities),
                "exploitation_steps": len(plan.get("exploitation_steps", [])),
                "risk_level": plan.get("risk_assessment", {}).get("overall_risk", "unknown")
            },
            tokens_used=llm_response.tokens_used,
            cost_usd=llm_response.metadata.get("cost_usd", 0.0)
        )

        return result

    async def _execute_exploit(self, task: AgentTask) -> AgentResult:
        """
        Execute specific exploit (SIMULATED ONLY)

        This method simulates exploit execution for training/planning purposes.
        It does NOT execute real attacks.
        """
        exploit_spec = task.parameters.get("exploit", {})
        target = task.parameters.get("target")
        approval_required = task.parameters.get("require_approval", self.require_approval)

        # Check if approval is required
        if approval_required:
            approval_status = await self._check_approval(task.task_id, exploit_spec)
            if approval_status != ApprovalStatus.APPROVED:
                return AgentResult(
                    task_id=task.task_id,
                    agent_name=self.name,
                    status=AgentStatus.PENDING,
                    metadata={
                        "approval_status": approval_status.value,
                        "message": "Exploit execution requires approval"
                    }
                )

        # SIMULATION: Analyze exploit instead of executing
        prompt = f"""Analyze this exploitation attempt and predict the outcome:

Target: {target}
Exploit: {json.dumps(exploit_spec, indent=2)}

Provide detailed analysis in JSON format:
{{
    "exploitation_analysis": {{
        "exploit_type": "type",
        "target": "target system",
        "predicted_outcome": "success|partial|failure",
        "success_probability": 0.85,
        "detection_risk": "low|medium|high",
        "impact": "description of impact",
        "indicators_of_compromise": ["ioc1", "ioc2"],
        "execution_steps": [
            {{
                "step": 1,
                "action": "action description",
                "expected_result": "result",
                "commands": ["command1"],
                "validation": "how to verify"
            }}
        ],
        "verification_checks": ["check1", "check2"],
        "rollback_procedure": "how to undo if needed",
        "artifacts_created": ["artifact1", "artifact2"]
    }},
    "recommendations": [
        {{
            "phase": "post-exploitation",
            "action": "next action",
            "priority": "high|medium|low",
            "reasoning": "why"
        }}
    ]
}}"""

        llm_response = await self.llm_client.generate_with_routing(
            prompt=prompt,
            task_type="analyze_exploitation_attempt",
            force_json=True,
            max_tokens=2048
        )

        try:
            analysis = json.loads(llm_response.content)
        except json.JSONDecodeError:
            import re
            json_match = re.search(r'\{.*\}', llm_response.content, re.DOTALL)
            if json_match:
                analysis = json.loads(json_match.group(0))
            else:
                analysis = {"exploitation_analysis": {"predicted_outcome": "failure"}}

        # Record execution
        self.executed_exploits[task.task_id] = {
            "exploit": exploit_spec,
            "target": target,
            "analysis": analysis,
            "timestamp": task.started_at
        }

        result = AgentResult(
            task_id=task.task_id,
            agent_name=self.name,
            status=AgentStatus.COMPLETED,
            findings=[analysis.get("exploitation_analysis", {})],
            recommendations=analysis.get("recommendations", []),
            metadata={
                "target": target,
                "exploit_type": exploit_spec.get("type", "unknown"),
                "predicted_outcome": analysis.get("exploitation_analysis", {}).get("predicted_outcome", "unknown"),
                "detection_risk": analysis.get("exploitation_analysis", {}).get("detection_risk", "unknown"),
                "simulation_mode": True
            },
            tokens_used=llm_response.tokens_used,
            cost_usd=llm_response.metadata.get("cost_usd", 0.0)
        )

        return result

    async def _verify_access(self, task: AgentTask) -> AgentResult:
        """
        Verify successful exploitation and access level

        Analyzes indicators to determine if exploitation was successful
        """
        target = task.parameters.get("target")
        exploit_task_id = task.parameters.get("exploit_task_id")
        indicators = task.parameters.get("indicators", {})

        # Get execution record
        exploit_record = self.executed_exploits.get(exploit_task_id, {})

        prompt = f"""Analyze these indicators to verify exploitation success:

Target: {target}
Exploit Executed: {json.dumps(exploit_record.get('exploit', {}), indent=2)}
Indicators: {json.dumps(indicators, indent=2)}

Provide verification analysis in JSON format:
{{
    "verification_result": {{
        "exploitation_successful": true,
        "access_level": "user|admin|system|root|none",
        "confidence": 0.95,
        "evidence": [
            {{
                "indicator": "indicator type",
                "value": "indicator value",
                "supports_success": true,
                "weight": "high|medium|low"
            }}
        ],
        "access_capabilities": [
            "capability1",
            "capability2"
        ],
        "limitations": [
            "limitation1"
        ],
        "persistence_achieved": false,
        "detection_status": "undetected|possibly_detected|detected"
    }},
    "next_steps": [
        "action1",
        "action2"
    ]
}}"""

        llm_response = await self.llm_client.generate_with_routing(
            prompt=prompt,
            task_type="verify_exploitation",
            force_json=True
        )

        try:
            verification = json.loads(llm_response.content)
        except json.JSONDecodeError:
            import re
            json_match = re.search(r'\{.*\}', llm_response.content, re.DOTALL)
            if json_match:
                verification = json.loads(json_match.group(0))
            else:
                verification = {"verification_result": {"exploitation_successful": False}}

        result = AgentResult(
            task_id=task.task_id,
            agent_name=self.name,
            status=AgentStatus.COMPLETED,
            findings=[verification.get("verification_result", {})],
            recommendations=verification.get("next_steps", []),
            metadata={
                "target": target,
                "exploitation_successful": verification.get("verification_result", {}).get("exploitation_successful", False),
                "access_level": verification.get("verification_result", {}).get("access_level", "none"),
                "confidence": verification.get("verification_result", {}).get("confidence", 0.0)
            },
            tokens_used=llm_response.tokens_used,
            cost_usd=llm_response.metadata.get("cost_usd", 0.0)
        )

        return result

    async def _recommend_post_exploitation(self, task: AgentTask) -> AgentResult:
        """
        Recommend post-exploitation actions

        Suggests next steps after successful exploitation
        """
        target = task.parameters.get("target")
        access_level = task.parameters.get("access_level", "user")
        objectives = task.parameters.get("objectives", [])
        current_position = task.parameters.get("current_position", {})

        prompt = f"""Recommend post-exploitation actions:

Target: {target}
Current Access Level: {access_level}
Objectives: {json.dumps(objectives, indent=2)}
Current Position: {json.dumps(current_position, indent=2)}

Provide recommendations in JSON format:
{{
    "post_exploitation_plan": {{
        "phase": "post-exploitation",
        "objectives": ["objective1", "objective2"],
        "actions": [
            {{
                "action_id": "PE1",
                "action": "action description",
                "category": "enumeration|privilege_escalation|lateral_movement|data_exfiltration|persistence",
                "priority": "high|medium|low",
                "mitre_technique": "T1234",
                "commands": ["command1"],
                "prerequisites": ["prerequisite1"],
                "risk": "low|medium|high",
                "detection_likelihood": "low|medium|high",
                "expected_outcome": "outcome description"
            }}
        ],
        "privilege_escalation_paths": [
            {{
                "path": "path description",
                "techniques": ["technique1"],
                "success_probability": 0.7,
                "risk": "low|medium|high"
            }}
        ],
        "lateral_movement_targets": [
            {{
                "target": "system/service",
                "method": "method description",
                "value": "why target this",
                "difficulty": "easy|medium|hard"
            }}
        ],
        "data_collection_priorities": [
            "data type 1",
            "data type 2"
        ]
    }}
}}"""

        llm_response = await self.llm_client.generate_with_routing(
            prompt=prompt,
            task_type="plan_post_exploitation",
            force_json=True,
            max_tokens=3072
        )

        try:
            recommendations = json.loads(llm_response.content)
        except json.JSONDecodeError:
            import re
            json_match = re.search(r'\{.*\}', llm_response.content, re.DOTALL)
            if json_match:
                recommendations = json.loads(json_match.group(0))
            else:
                recommendations = {"post_exploitation_plan": {"actions": []}}

        plan = recommendations.get("post_exploitation_plan", {})

        result = AgentResult(
            task_id=task.task_id,
            agent_name=self.name,
            status=AgentStatus.COMPLETED,
            findings=[plan],
            recommendations=plan.get("actions", []),
            metadata={
                "target": target,
                "access_level": access_level,
                "actions_recommended": len(plan.get("actions", [])),
                "privilege_escalation_paths": len(plan.get("privilege_escalation_paths", [])),
                "lateral_movement_targets": len(plan.get("lateral_movement_targets", []))
            },
            tokens_used=llm_response.tokens_used,
            cost_usd=llm_response.metadata.get("cost_usd", 0.0)
        )

        return result

    async def _check_approval(self, task_id: str, exploit_spec: Dict[str, Any]) -> ApprovalStatus:
        """
        Check if exploit requires approval and current approval status

        In production, this would interface with a human approval system
        For now, returns AUTO_APPROVED for simulation mode
        """
        # Check if already approved
        if task_id in self.pending_approvals:
            return ApprovalStatus[self.pending_approvals[task_id].get("status", "PENDING")]

        # Auto-approve for simulation/testing
        risk_level = exploit_spec.get("risk_level", "medium")

        if risk_level == "low":
            return ApprovalStatus.AUTO_APPROVED

        # Store pending approval
        self.pending_approvals[task_id] = {
            "exploit": exploit_spec,
            "status": ApprovalStatus.PENDING.value,
            "requested_at": task_id
        }

        # In simulation mode, auto-approve after logging
        self.logger.info(f"Exploit approval requested for task {task_id}: {exploit_spec.get('type', 'unknown')}")
        self.pending_approvals[task_id]["status"] = ApprovalStatus.AUTO_APPROVED.value

        return ApprovalStatus.AUTO_APPROVED

    def _build_exploitation_plan_prompt(
        self,
        vulnerabilities: List[Dict[str, Any]],
        target: str,
        constraints: Dict[str, Any],
        context: Dict[str, Any]
    ) -> str:
        """Build exploitation planning prompt"""
        prompt = f"""You are an exploitation planning expert. Create an exploitation plan:

Target: {target}

Vulnerabilities:
{json.dumps(vulnerabilities[:10], indent=2)}

Constraints:
{json.dumps(constraints, indent=2)}

"""

        # Add context if available
        if context.get("exploitation_techniques"):
            prompt += "\nRelevant MITRE ATT&CK Exploitation Techniques:\n"
            for tech in context["exploitation_techniques"][:5]:
                prompt += f"- {tech['technique_id']}: {tech['technique_name']}\n"

        if context.get("available_exploits"):
            prompt += "\nKnown Exploits:\n"
            for exploit in context["available_exploits"][:3]:
                prompt += f"- {exploit['exploit_id']}: {exploit['description'][:100]}...\n"

        prompt += """

Provide exploitation plan in JSON format:
{
    "exploitation_plan": {
        "strategy": "strategy description",
        "target_vulnerability": "primary vulnerability to exploit",
        "exploitation_steps": [
            {
                "step": 1,
                "phase": "reconnaissance|weaponization|delivery|exploitation|installation|command_and_control|actions_on_objectives",
                "action": "action description",
                "tool": "tool name",
                "commands": ["command1"],
                "mitre_technique": "T1234",
                "expected_result": "result",
                "success_criteria": "how to verify",
                "fallback": "what if this fails",
                "risk_level": "low|medium|high",
                "detection_likelihood": "low|medium|high"
            }
        ],
        "success_probability": 0.75,
        "estimated_duration": "time estimate",
        "required_tools": ["tool1", "tool2"],
        "required_access": "network|local|credentials",
        "risk_assessment": {
            "overall_risk": "low|medium|high|critical",
            "technical_risk": "description",
            "operational_risk": "description",
            "legal_compliance": "compliant|review_needed|non_compliant",
            "mitigation_steps": ["step1", "step2"]
        },
        "contingency_plans": [
            {
                "scenario": "scenario description",
                "response": "what to do"
            }
        ]
    }
}"""

        return prompt

    def get_exploitation_history(self) -> List[Dict[str, Any]]:
        """Get history of executed exploits"""
        return list(self.executed_exploits.values())

    def get_pending_approvals(self) -> List[Dict[str, Any]]:
        """Get pending approval requests"""
        return [
            {"task_id": task_id, **details}
            for task_id, details in self.pending_approvals.items()
            if details.get("status") == ApprovalStatus.PENDING.value
        ]
