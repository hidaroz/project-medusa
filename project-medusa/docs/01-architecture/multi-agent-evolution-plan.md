# MEDUSA Multi-Agent Evolution: Implementation Plan

**Navigation**: [Home](../../README.md) â†’ [Docs](../INDEX.md) â†’ [Architecture](README.md) â†’ Multi-Agent Evolution Plan

---

## ðŸ“Š Implementation Status Update (November 2025)

**Overall Status**: âœ… **Phases 1-2 COMPLETE** | ðŸ”„ **Phase 3 In Progress (40%)**

### Quick Status Overview

| Component | Status | Notes |
|-----------|--------|-------|
| **AWS Bedrock Integration** | âœ… Complete | Claude 3.5 Sonnet/Haiku in production |
| **Multi-Agent System** | âœ… Complete | 6 agents orchestrated successfully |
| **Context Fusion Engine** | âœ… Complete | Neo4j + ChromaDB integrated |
| **Smart Model Routing** | âœ… Complete | 60-70% cost savings achieved |
| **Cost Tracking** | âœ… Complete | Real-time per-agent tracking |
| **Multi-Format Reporting** | âœ… Complete | Executive/Technical/Remediation |
| **Vector Database** | âœ… Complete | MITRE/CVE/Tools indexed |
| **Graph Database** | âœ… Complete | Infrastructure state tracking |
| **Web Dashboard** | ðŸ”„ In Progress | Planned for Q1 2026 |
| **Custom Agent Training** | ðŸ“‹ Planned | RAG fine-tuning research phase |

### Achievements (November 2025)

âœ… **Production-Ready Multi-Agent System**
- 6 specialized agents working in coordination
- Orchestrator successfully delegates and aggregates
- Context fusion provides intelligent recommendations

âœ… **Cost-Optimized LLM Usage**
- Smart routing between Sonnet (complex) and Haiku (simple) tasks
- Average 62% cost savings vs Sonnet-only
- Real-time cost tracking per agent per operation

âœ… **Enterprise Features**
- AWS Bedrock integration with automatic failover
- Comprehensive reporting in multiple formats
- MITRE ATT&CK framework integration
- CVE correlation engine

---

## ðŸ“‹ Executive Summary

This document outlines the comprehensive architectural evolution of MEDUSA from a single-agent system to a sophisticated multi-agent orchestration platform with AWS Bedrock integration, dual-database context engineering (Vector + Graph), and intelligent agent specialization.

**Strategic Goals**:
1. â˜ï¸ **AWS Bedrock Integration** - Cloud-native LLM with cost tracking
2. ðŸ§  **Context Fusion Engine** - Vector DB + Graph DB for intelligent context
3. ðŸ¤– **Multi-Agent System** - Specialized agents with orchestrated collaboration

**Timeline**: 8-12 weeks (3 major phases)

**Impact**: Transform MEDUSA from an educational prototype to production-grade AI-powered security platform

---

## ðŸŽ¯ Design Philosophy

### Core Principles

1. **Backward Compatibility**: Existing CLI modes must continue to work
2. **Incremental Deployment**: Each phase delivers standalone value
3. **Cost Consciousness**: Track and optimize LLM token usage
4. **Air-Gap Capability**: Maintain offline/local operation mode
5. **Safety First**: Approval gates remain non-negotiable

### Architecture Tenets

- **Single Source of Truth**: Neo4j graph for all infrastructure state
- **Knowledge Separation**: Vector DB for static knowledge, Graph DB for dynamic state
- **Agent Autonomy**: Each agent owns its domain but reports to orchestrator
- **Observability**: Full logging, metrics, and cost tracking per operation

---

## ðŸ“ Target Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MEDUSA ORCHESTRATION LAYER                  â”‚
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Orchestrator Agent (Supervisor)                â”‚  â”‚
â”‚  â”‚         Model: Claude 4.5 Sonnet (Bedrock)               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                                          â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚       â”‚             â”‚             â”‚              â”‚             â”‚
â”‚       â–¼             â–¼             â–¼              â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Recon   â”‚  â”‚ Vuln    â”‚  â”‚ Exploit  â”‚  â”‚ Planning â”‚          â”‚
â”‚  â”‚ Agent   â”‚  â”‚ Analysisâ”‚  â”‚ Agent    â”‚  â”‚ Agent    â”‚          â”‚
â”‚  â”‚ (Haiku) â”‚  â”‚ (Haiku) â”‚  â”‚ (Haiku)  â”‚  â”‚ (Sonnet) â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
â”‚       â”‚            â”‚             â”‚             â”‚               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                    â”‚             â”‚                             â”‚
â”‚                    â–¼             â–¼                             â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚       â”‚    Context Fusion Engine (NEW)     â”‚                   â”‚
â”‚       â”‚                                    â”‚                   â”‚
â”‚       â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                   â”‚
â”‚       â”‚  â”‚ Vector Store â”‚  â”‚  Neo4j      â”‚ â”‚                   â”‚
â”‚       â”‚  â”‚  (Chroma)    â”‚  â”‚  Graph DB   â”‚ â”‚                   â”‚
â”‚       â”‚  â”‚              â”‚  â”‚             â”‚ â”‚                   â”‚
â”‚       â”‚  â”‚ â€¢ MITRE      â”‚  â”‚ â€¢ Hosts     â”‚ â”‚                   â”‚
â”‚       â”‚  â”‚ â€¢ CVEs       â”‚  â”‚ â€¢ Vulns     â”‚ â”‚                   â”‚
â”‚       â”‚  â”‚ â€¢ Tool Docs  â”‚  â”‚ â€¢ Users     â”‚ â”‚                   â”‚
â”‚       â”‚  â”‚ â€¢ History    â”‚  â”‚ â€¢ Ports     â”‚ â”‚                   â”‚
â”‚       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                   â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AWS BEDROCK INTEGRATION                     â”‚
â”‚                                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Claude 4.5   â”‚  â”‚ Claude 4.5   â”‚  â”‚ Titan           â”‚       â”‚
â”‚  â”‚ Sonnet       â”‚  â”‚ Haiku        â”‚  â”‚ Embeddings      â”‚       â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚                 â”‚       â”‚
â”‚  â”‚ â€¢ Orchestr.  â”‚  â”‚ â€¢ Recon      â”‚  â”‚ â€¢ Vector DB     â”‚       â”‚
â”‚  â”‚ â€¢ Planning   â”‚  â”‚ â€¢ Analysis   â”‚  â”‚   indexing      â”‚       â”‚
â”‚  â”‚ â€¢ Strategy   â”‚  â”‚ â€¢ Exploit    â”‚  â”‚ â€¢ Semantic      â”‚       â”‚
â”‚  â”‚              â”‚  â”‚ â€¢ Reporting  â”‚  â”‚   search        â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                                â”‚
â”‚  Fallback: Ollama (Local) for air-gapped operations            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸš€ Phase 1: AWS Bedrock Integration (Weeks 1-3)

### Phase 1.1: Bedrock Provider Foundation (Week 1)

**Objective**: Add AWS Bedrock as a first-class LLM provider with cost tracking

#### Tasks

**1.1.1 Create Bedrock Provider Class**

Location: `medusa-cli/src/medusa/core/llm/providers/bedrock.py`

```python
"""
AWS Bedrock LLM Provider
Supports Claude 3.5 (Sonnet, Haiku), Titan, and other Bedrock models
"""

import boto3
import json
import logging
from typing import Optional, Dict, Any
from botocore.exceptions import ClientError

from .base import BaseLLMProvider, LLMResponse
from ..config import LLMConfig
from ..exceptions import LLMError, LLMAuthenticationError, LLMRateLimitError


class BedrockProvider(BaseLLMProvider):
    """AWS Bedrock provider with cost tracking"""

    PROVIDER_NAME = "bedrock"

    # Model pricing (per 1M tokens) - as of 2025
    PRICING = {
        "anthropic.claude-3-5-sonnet-20241022-v2:0": {
            "input": 3.00,  # $3 per 1M input tokens
            "output": 15.00  # $15 per 1M output tokens
        },
        "anthropic.claude-3-5-haiku-20241022-v1:0": {
            "input": 0.80,  # $0.80 per 1M input tokens
            "output": 4.00  # $4 per 1M output tokens
        },
        "amazon.titan-text-premier-v1:0": {
            "input": 0.50,
            "output": 1.50
        }
    }

    def __init__(self, config: LLMConfig):
        """
        Initialize Bedrock provider

        Args:
            config: LLM configuration with AWS credentials
        """
        super().__init__(config)

        # Model selection: smart vs fast
        self.model = config.cloud_model or "anthropic.claude-3-5-haiku-20241022-v1:0"

        # Initialize boto3 client
        self.bedrock_runtime = boto3.client(
            service_name='bedrock-runtime',
            region_name=config.aws_region or 'us-west-2',
            aws_access_key_id=config.aws_access_key_id,
            aws_secret_access_key=config.aws_secret_access_key
        )

        # Cost tracking
        self.total_cost = 0.0
        self.total_input_tokens = 0
        self.total_output_tokens = 0

        self.logger = logging.getLogger(__name__)
        self.logger.info(f"BedrockProvider initialized with model={self.model}")

    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        force_json: bool = False
    ) -> LLMResponse:
        """
        Generate completion using AWS Bedrock

        Returns:
            LLMResponse with content, metadata, and cost tracking
        """
        import time
        start_time = time.time()

        # Build request body based on model family
        if self.model.startswith("anthropic.claude"):
            body = self._build_claude_request(
                prompt, system_prompt, temperature, max_tokens, force_json
            )
        elif self.model.startswith("amazon.titan"):
            body = self._build_titan_request(
                prompt, system_prompt, temperature, max_tokens
            )
        else:
            raise LLMError(f"Unsupported model: {self.model}")

        try:
            response = self.bedrock_runtime.invoke_model(
                modelId=self.model,
                body=json.dumps(body)
            )

            response_body = json.loads(response['body'].read())

            # Parse response based on model
            content, input_tokens, output_tokens = self._parse_response(response_body)

            # Calculate cost
            cost = self._calculate_cost(input_tokens, output_tokens)

            # Update running totals
            self.total_input_tokens += input_tokens
            self.total_output_tokens += output_tokens
            self.total_cost += cost

            latency_ms = (time.time() - start_time) * 1000

            self.logger.info(
                f"Bedrock response: tokens={input_tokens}+{output_tokens}, "
                f"cost=${cost:.4f}, latency={latency_ms:.0f}ms"
            )

            return LLMResponse(
                content=content,
                provider=self.PROVIDER_NAME,
                model=self.model,
                tokens_used=input_tokens + output_tokens,
                latency_ms=latency_ms,
                metadata={
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "cost_usd": cost,
                    "cumulative_cost_usd": self.total_cost,
                    "model_id": self.model
                }
            )

        except ClientError as e:
            error_code = e.response['Error']['Code']
            if error_code == 'ThrottlingException':
                raise LLMRateLimitError(f"Bedrock rate limit: {e}")
            elif error_code in ['AccessDeniedException', 'UnauthorizedException']:
                raise LLMAuthenticationError(f"Bedrock auth failed: {e}")
            else:
                raise LLMError(f"Bedrock error: {e}")

    def _build_claude_request(
        self,
        prompt: str,
        system_prompt: Optional[str],
        temperature: Optional[float],
        max_tokens: Optional[int],
        force_json: bool
    ) -> Dict[str, Any]:
        """Build request body for Claude models"""
        messages = [{"role": "user", "content": prompt}]

        body = {
            "anthropic_version": "bedrock-2023-05-31",
            "messages": messages,
            "temperature": temperature or self.config.temperature,
            "max_tokens": max_tokens or self.config.max_tokens
        }

        if system_prompt:
            body["system"] = system_prompt

        if force_json:
            body["system"] = (body.get("system", "") +
                             "\n\nYou must respond with valid JSON only.")

        return body

    def _build_titan_request(
        self,
        prompt: str,
        system_prompt: Optional[str],
        temperature: Optional[float],
        max_tokens: Optional[int]
    ) -> Dict[str, Any]:
        """Build request body for Titan models"""
        full_prompt = prompt
        if system_prompt:
            full_prompt = f"{system_prompt}\n\n{prompt}"

        return {
            "inputText": full_prompt,
            "textGenerationConfig": {
                "temperature": temperature or self.config.temperature,
                "maxTokenCount": max_tokens or self.config.max_tokens,
                "topP": 0.9
            }
        }

    def _parse_response(self, response_body: Dict[str, Any]) -> tuple:
        """Parse response and extract content + token counts"""
        if "content" in response_body:  # Claude format
            content = response_body["content"][0]["text"]
            input_tokens = response_body["usage"]["input_tokens"]
            output_tokens = response_body["usage"]["output_tokens"]
        elif "results" in response_body:  # Titan format
            content = response_body["results"][0]["outputText"]
            input_tokens = response_body.get("inputTextTokenCount", 0)
            output_tokens = response_body.get("results", [{}])[0].get("tokenCount", 0)
        else:
            raise LLMError("Unexpected response format from Bedrock")

        return content, input_tokens, output_tokens

    def _calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
        """Calculate cost in USD for this request"""
        pricing = self.PRICING.get(self.model, {"input": 0, "output": 0})

        input_cost = (input_tokens / 1_000_000) * pricing["input"]
        output_cost = (output_tokens / 1_000_000) * pricing["output"]

        return input_cost + output_cost

    async def health_check(self) -> bool:
        """Check if Bedrock is accessible"""
        try:
            # Try listing models as a connectivity test
            bedrock_client = boto3.client('bedrock', region_name='us-west-2')
            bedrock_client.list_foundation_models(
                byProvider='anthropic',
                byOutputModality='TEXT'
            )
            return True
        except Exception as e:
            self.logger.error(f"Bedrock health check failed: {e}")
            return False

    async def get_model_info(self) -> Dict[str, Any]:
        """Get model information"""
        pricing = self.PRICING.get(self.model, {})

        return {
            "model_id": self.model,
            "provider": "AWS Bedrock",
            "pricing": {
                "input_per_1m": pricing.get("input", 0),
                "output_per_1m": pricing.get("output", 0),
                "currency": "USD"
            },
            "session_stats": {
                "total_input_tokens": self.total_input_tokens,
                "total_output_tokens": self.total_output_tokens,
                "total_cost_usd": self.total_cost
            }
        }

    def get_cost_summary(self) -> Dict[str, Any]:
        """Get detailed cost breakdown for current session"""
        return {
            "provider": self.PROVIDER_NAME,
            "model": self.model,
            "input_tokens": self.total_input_tokens,
            "output_tokens": self.total_output_tokens,
            "total_tokens": self.total_input_tokens + self.total_output_tokens,
            "total_cost_usd": self.total_cost,
            "pricing": self.PRICING.get(self.model, {})
        }
```

**1.1.2 Update LLM Configuration**

Location: `medusa-cli/src/medusa/core/llm/config.py`

Add new fields:
```python
@dataclass
class LLMConfig:
    # ... existing fields ...

    # AWS Bedrock configuration
    aws_region: Optional[str] = None
    aws_access_key_id: Optional[str] = None
    aws_secret_access_key: Optional[str] = None

    # Model selection strategy
    smart_model: str = "anthropic.claude-3-5-sonnet-20241022-v2:0"  # For reasoning
    fast_model: str = "anthropic.claude-3-5-haiku-20241022-v1:0"    # For tools
```

**1.1.3 Update Factory Pattern**

Location: `medusa-cli/src/medusa/core/llm/factory.py`

```python
from .providers.bedrock import BedrockProvider

def create_llm_provider(config: LLMConfig) -> BaseLLMProvider:
    """Factory function to create LLM provider"""

    if config.provider == "bedrock":
        return BedrockProvider(config)
    elif config.provider == "auto":
        # Try Bedrock first, fallback to local
        try:
            provider = BedrockProvider(config)
            if asyncio.run(provider.health_check()):
                return provider
        except Exception:
            pass
        # Fallback to local
        return LocalProvider(config)
    # ... existing providers ...
```

**1.1.4 Add Dependencies**

Update `medusa-cli/requirements.txt`:
```txt
boto3>=1.34.0
botocore>=1.34.0
```

**1.1.5 Configuration File Updates**

Update `~/.medusa/config.yaml` template:
```yaml
llm:
  provider: bedrock  # Options: bedrock, local, openai, anthropic, auto

  # AWS Bedrock configuration
  aws:
    region: us-west-2
    # Credentials via environment variables or AWS config
    # AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY

  # Model selection strategy
  models:
    smart: anthropic.claude-3-5-sonnet-20241022-v2:0  # Planning, orchestration
    fast: anthropic.claude-3-5-haiku-20241022-v1:0    # Tool execution

  # Fallback configuration
  fallback:
    provider: local
    model: mistral:7b-instruct
```

#### Deliverables

- âœ… BedrockProvider fully implemented with cost tracking
- âœ… Factory pattern updated with Bedrock support
- âœ… Health checks and error handling
- âœ… Configuration schema extended
- âœ… Unit tests for Bedrock provider
- âœ… Integration test with real Bedrock API

#### Testing

```bash
# Unit tests
pytest medusa-cli/tests/unit/test_bedrock_provider.py

# Integration test (requires AWS credentials)
pytest medusa-cli/tests/integration/test_bedrock_integration.py

# Cost tracking validation
pytest medusa-cli/tests/unit/test_cost_tracking.py
```

---

### Phase 1.2: Smart Model Routing (Week 2)

**Objective**: Implement intelligent model selection based on task complexity

#### Tasks

**1.2.1 Create Model Router**

Location: `medusa-cli/src/medusa/core/llm/router.py`

```python
"""
Intelligent LLM model routing
Routes tasks to appropriate models based on complexity and cost
"""

from enum import Enum
from typing import Dict, Any
from .config import LLMConfig
from .providers.base import BaseLLMProvider


class TaskComplexity(Enum):
    """Task complexity levels for model routing"""
    SIMPLE = "simple"       # Tool parsing, data extraction
    MODERATE = "moderate"   # Analysis, recommendations
    COMPLEX = "complex"     # Strategic planning, multi-step reasoning


class ModelRouter:
    """
    Routes LLM requests to appropriate models based on task complexity

    Strategy:
    - SIMPLE tasks â†’ Haiku (fast, cheap)
    - MODERATE tasks â†’ Haiku (still capable)
    - COMPLEX tasks â†’ Sonnet (deep reasoning)
    """

    def __init__(self, config: LLMConfig):
        self.config = config
        self.smart_model = config.smart_model
        self.fast_model = config.fast_model

    def select_model(self, task_type: str, context: Dict[str, Any] = None) -> str:
        """
        Select appropriate model for task

        Args:
            task_type: Type of task (e.g., "parse_nmap", "plan_attack")
            context: Additional context for routing decision

        Returns:
            Model identifier string
        """
        complexity = self._assess_complexity(task_type, context)

        if complexity == TaskComplexity.COMPLEX:
            return self.smart_model
        else:
            return self.fast_model

    def _assess_complexity(
        self,
        task_type: str,
        context: Dict[str, Any] = None
    ) -> TaskComplexity:
        """Assess task complexity"""

        # Complex tasks requiring deep reasoning
        complex_tasks = {
            "orchestrate_operation",
            "plan_attack_strategy",
            "assess_risk_holistic",
            "generate_executive_report",
            "analyze_attack_graph"
        }

        # Simple tasks - tool parsing, extraction
        simple_tasks = {
            "parse_nmap_output",
            "extract_vulnerabilities",
            "format_report",
            "validate_target",
            "check_tool_availability"
        }

        if task_type in complex_tasks:
            return TaskComplexity.COMPLEX
        elif task_type in simple_tasks:
            return TaskComplexity.SIMPLE
        else:
            return TaskComplexity.MODERATE
```

**1.2.2 Update LLMClient with Routing**

Location: `medusa-cli/src/medusa/core/llm/client.py`

```python
class LLMClient:
    def __init__(self, config: LLMConfig, provider: BaseLLMProvider):
        self.config = config
        self.provider = provider
        self.router = ModelRouter(config)  # NEW

    async def generate_with_routing(
        self,
        prompt: str,
        task_type: str,
        **kwargs
    ) -> LLMResponse:
        """
        Generate with automatic model routing

        Args:
            prompt: User prompt
            task_type: Task identifier for routing
        """
        # Select appropriate model
        selected_model = self.router.select_model(task_type)

        # Update provider model if different
        original_model = self.provider.model
        if hasattr(self.provider, 'model') and selected_model != original_model:
            self.provider.model = selected_model
            self.logger.info(f"Routing to {selected_model} for task={task_type}")

        try:
            response = await self.generate(prompt, **kwargs)
            return response
        finally:
            # Restore original model
            if hasattr(self.provider, 'model'):
                self.provider.model = original_model
```

#### Deliverables

- âœ… ModelRouter implementation
- âœ… Task complexity assessment logic
- âœ… LLMClient integration with routing
- âœ… Cost comparison metrics
- âœ… Documentation on routing strategy

---

### Phase 1.3: Cost Tracking & Reporting (Week 3)

**Objective**: Comprehensive cost tracking and reporting per operation

#### Tasks

**1.3.1 Operation Cost Tracker**

Location: `medusa-cli/src/medusa/core/cost_tracker.py`

```python
"""
Operation-level cost tracking and reporting
Tracks LLM usage, tokens, and costs per operation
"""

from datetime import datetime
from typing import Dict, Any, List
from dataclasses import dataclass, field
import json


@dataclass
class CostEntry:
    """Single cost entry for an LLM call"""
    timestamp: datetime
    agent: str
    task_type: str
    model: str
    input_tokens: int
    output_tokens: int
    cost_usd: float
    latency_ms: float
    metadata: Dict[str, Any] = field(default_factory=dict)


class OperationCostTracker:
    """Track costs for entire operation"""

    def __init__(self, operation_id: str):
        self.operation_id = operation_id
        self.entries: List[CostEntry] = []
        self.start_time = datetime.now()
        self.end_time = None

    def record(
        self,
        agent: str,
        task_type: str,
        response: 'LLMResponse'
    ):
        """Record a cost entry"""
        entry = CostEntry(
            timestamp=datetime.now(),
            agent=agent,
            task_type=task_type,
            model=response.model,
            input_tokens=response.metadata.get("input_tokens", 0),
            output_tokens=response.metadata.get("output_tokens", 0),
            cost_usd=response.metadata.get("cost_usd", 0.0),
            latency_ms=response.latency_ms,
            metadata=response.metadata
        )
        self.entries.append(entry)

    def finalize(self):
        """Mark operation as complete"""
        self.end_time = datetime.now()

    def get_summary(self) -> Dict[str, Any]:
        """Get cost summary"""
        total_cost = sum(e.cost_usd for e in self.entries)
        total_tokens = sum(e.input_tokens + e.output_tokens for e in self.entries)
        total_input = sum(e.input_tokens for e in self.entries)
        total_output = sum(e.output_tokens for e in self.entries)

        # Cost by agent
        agent_costs = {}
        for entry in self.entries:
            if entry.agent not in agent_costs:
                agent_costs[entry.agent] = 0.0
            agent_costs[entry.agent] += entry.cost_usd

        # Cost by model
        model_costs = {}
        for entry in self.entries:
            if entry.model not in model_costs:
                model_costs[entry.model] = {"calls": 0, "cost": 0.0}
            model_costs[entry.model]["calls"] += 1
            model_costs[entry.model]["cost"] += entry.cost_usd

        duration = (self.end_time - self.start_time).total_seconds() if self.end_time else 0

        return {
            "operation_id": self.operation_id,
            "start_time": self.start_time.isoformat(),
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "duration_seconds": duration,
            "total_cost_usd": total_cost,
            "total_tokens": total_tokens,
            "input_tokens": total_input,
            "output_tokens": total_output,
            "total_calls": len(self.entries),
            "agent_breakdown": agent_costs,
            "model_breakdown": model_costs,
            "average_latency_ms": sum(e.latency_ms for e in self.entries) / len(self.entries) if self.entries else 0
        }

    def export_json(self, filepath: str):
        """Export cost data to JSON"""
        data = {
            "summary": self.get_summary(),
            "entries": [
                {
                    "timestamp": e.timestamp.isoformat(),
                    "agent": e.agent,
                    "task_type": e.task_type,
                    "model": e.model,
                    "input_tokens": e.input_tokens,
                    "output_tokens": e.output_tokens,
                    "cost_usd": e.cost_usd,
                    "latency_ms": e.latency_ms
                }
                for e in self.entries
            ]
        }

        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)
```

**1.3.2 CLI Cost Reporting**

Add to reporter output:

```python
# In medusa-cli/src/medusa/reporter.py

def generate_html_report(self, operation_data: Dict[str, Any]):
    """Enhanced report with cost tracking"""

    # ... existing code ...

    # Add cost section
    if "cost_summary" in operation_data:
        cost_html = self._render_cost_section(operation_data["cost_summary"])
        report_html += cost_html
```

#### Deliverables

- âœ… OperationCostTracker implementation
- âœ… Per-agent cost breakdown
- âœ… Per-model cost breakdown
- âœ… JSON export for cost data
- âœ… HTML report integration
- âœ… CLI command: `medusa cost-report --operation-id <id>`

---

## ðŸ§  Phase 2: Context Fusion Engine (Weeks 4-7)

### Phase 2.1: Vector Database Foundation (Week 4)

**Objective**: Set up Chroma vector database with Titan Embeddings

#### Tasks

**2.1.1 Chroma Setup & Configuration**

Location: `medusa-cli/src/medusa/context/vector_store.py`

```python
"""
Vector Store for semantic knowledge retrieval
Uses ChromaDB with AWS Titan Embeddings (or local sentence-transformers)
"""

import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any, Optional
import logging
from pathlib import Path


class VectorStore:
    """
    Vector database for semantic search over security knowledge

    Stores:
    - MITRE ATT&CK techniques
    - CVE database
    - Tool documentation
    - Historical operation summaries
    """

    def __init__(
        self,
        persist_directory: str = "~/.medusa/vector_db",
        embedding_provider: str = "local"  # or "bedrock"
    ):
        self.persist_dir = Path(persist_directory).expanduser()
        self.persist_dir.mkdir(parents=True, exist_ok=True)

        # Initialize ChromaDB
        self.client = chromadb.Client(Settings(
            persist_directory=str(self.persist_dir),
            anonymized_telemetry=False
        ))

        # Configure embedding function
        if embedding_provider == "bedrock":
            self.embedding_function = self._create_bedrock_embeddings()
        else:
            self.embedding_function = self._create_local_embeddings()

        # Collections
        self.collections = {
            "mitre_attack": self._get_or_create_collection("mitre_attack"),
            "cve_database": self._get_or_create_collection("cve_database"),
            "tool_docs": self._get_or_create_collection("tool_documentation"),
            "operation_history": self._get_or_create_collection("operation_history")
        }

        self.logger = logging.getLogger(__name__)
        self.logger.info(f"VectorStore initialized at {self.persist_dir}")

    def _get_or_create_collection(self, name: str):
        """Get or create a collection"""
        try:
            return self.client.get_collection(
                name=name,
                embedding_function=self.embedding_function
            )
        except ValueError:
            return self.client.create_collection(
                name=name,
                embedding_function=self.embedding_function,
                metadata={"hnsw:space": "cosine"}
            )

    def _create_bedrock_embeddings(self):
        """Create Bedrock Titan embedding function"""
        import boto3

        class BedrockEmbeddingFunction:
            def __init__(self):
                self.bedrock = boto3.client('bedrock-runtime', region_name='us-west-2')
                self.model_id = "amazon.titan-embed-text-v2:0"

            def __call__(self, texts: List[str]) -> List[List[float]]:
                embeddings = []
                for text in texts:
                    response = self.bedrock.invoke_model(
                        modelId=self.model_id,
                        body=json.dumps({"inputText": text})
                    )
                    result = json.loads(response['body'].read())
                    embeddings.append(result['embedding'])
                return embeddings

        return BedrockEmbeddingFunction()

    def _create_local_embeddings(self):
        """Create local sentence-transformers embedding function"""
        from chromadb.utils import embedding_functions

        return embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name="all-MiniLM-L6-v2"
        )

    def index_mitre_attack(self, techniques: List[Dict[str, Any]]):
        """
        Index MITRE ATT&CK techniques

        Args:
            techniques: List of MITRE techniques with id, name, description
        """
        collection = self.collections["mitre_attack"]

        ids = [t["id"] for t in techniques]
        documents = [
            f"{t['name']}: {t['description']}\n\nTactics: {', '.join(t.get('tactics', []))}"
            for t in techniques
        ]
        metadatas = [
            {
                "technique_id": t["id"],
                "technique_name": t["name"],
                "tactics": ",".join(t.get("tactics", [])),
                "platforms": ",".join(t.get("platforms", []))
            }
            for t in techniques
        ]

        collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )

        self.logger.info(f"Indexed {len(techniques)} MITRE ATT&CK techniques")

    def search_mitre_techniques(
        self,
        query: str,
        n_results: int = 5,
        filter_tactics: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        Semantic search for relevant MITRE techniques

        Args:
            query: Search query (e.g., "lateral movement using credentials")
            n_results: Number of results to return
            filter_tactics: Filter by specific tactics

        Returns:
            List of relevant techniques with scores
        """
        collection = self.collections["mitre_attack"]

        where_filter = {}
        if filter_tactics:
            # Note: Chroma doesn't support OR filters well, so we search all
            # and filter in post-processing
            pass

        results = collection.query(
            query_texts=[query],
            n_results=n_results,
            where=where_filter if where_filter else None
        )

        techniques = []
        for i in range(len(results['ids'][0])):
            techniques.append({
                "technique_id": results['metadatas'][0][i]['technique_id'],
                "technique_name": results['metadatas'][0][i]['technique_name'],
                "description": results['documents'][0][i],
                "relevance_score": 1.0 - results['distances'][0][i],  # Convert distance to similarity
                "tactics": results['metadatas'][0][i]['tactics'].split(',')
            })

        return techniques

    def index_tool_documentation(self, tool_docs: List[Dict[str, Any]]):
        """
        Index tool documentation for semantic search

        Args:
            tool_docs: List with tool, command, description, examples
        """
        collection = self.collections["tool_docs"]

        ids = [f"{doc['tool']}_{i}" for i, doc in enumerate(tool_docs)]
        documents = [
            f"Tool: {doc['tool']}\nCommand: {doc['command']}\n"
            f"Description: {doc['description']}\nExamples: {doc.get('examples', '')}"
            for doc in tool_docs
        ]
        metadatas = [
            {
                "tool": doc["tool"],
                "command": doc["command"],
                "category": doc.get("category", "general")
            }
            for doc in tool_docs
        ]

        collection.add(ids=ids, documents=documents, metadatas=metadatas)
        self.logger.info(f"Indexed {len(tool_docs)} tool documentation entries")

    def search_tool_usage(self, query: str, n_results: int = 3) -> List[Dict[str, Any]]:
        """
        Search for relevant tool usage examples

        Example query: "scan for SQL injection vulnerabilities"
        Returns: SQLMap commands and usage
        """
        collection = self.collections["tool_docs"]

        results = collection.query(
            query_texts=[query],
            n_results=n_results
        )

        tool_usage = []
        for i in range(len(results['ids'][0])):
            tool_usage.append({
                "tool": results['metadatas'][0][i]['tool'],
                "command": results['metadatas'][0][i]['command'],
                "documentation": results['documents'][0][i],
                "relevance_score": 1.0 - results['distances'][0][i]
            })

        return tool_usage

    def get_stats(self) -> Dict[str, Any]:
        """Get vector store statistics"""
        stats = {}
        for name, collection in self.collections.items():
            stats[name] = collection.count()

        return {
            "persist_directory": str(self.persist_dir),
            "collections": stats,
            "total_documents": sum(stats.values())
        }
```

**2.1.2 MITRE ATT&CK Indexer**

Location: `medusa-cli/scripts/index_mitre_attack.py`

```python
"""
Index MITRE ATT&CK framework into vector database
Downloads latest MITRE data and indexes for semantic search
"""

import requests
import json
from medusa.context.vector_store import VectorStore


def download_mitre_attack() -> List[Dict[str, Any]]:
    """Download MITRE ATT&CK Enterprise matrix"""
    url = "https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json"

    response = requests.get(url)
    data = response.json()

    techniques = []
    for obj in data["objects"]:
        if obj["type"] == "attack-pattern":
            techniques.append({
                "id": obj["external_references"][0]["external_id"],
                "name": obj["name"],
                "description": obj["description"],
                "tactics": [phase["phase_name"] for phase in obj.get("kill_chain_phases", [])],
                "platforms": obj.get("x_mitre_platforms", [])
            })

    return techniques


if __name__ == "__main__":
    print("Downloading MITRE ATT&CK framework...")
    techniques = download_mitre_attack()
    print(f"Downloaded {len(techniques)} techniques")

    print("Indexing into vector store...")
    vector_store = VectorStore()
    vector_store.index_mitre_attack(techniques)

    print("âœ… MITRE ATT&CK indexed successfully")
    print(vector_store.get_stats())
```

#### Deliverables

- âœ… VectorStore implementation with ChromaDB
- âœ… Bedrock Titan Embeddings integration
- âœ… Local sentence-transformers fallback
- âœ… MITRE ATT&CK indexing script
- âœ… Tool documentation indexing
- âœ… Unit tests for vector operations
- âœ… Performance benchmarks

---

### Phase 2.2: Context Fusion Engine (Week 5)

**Objective**: Combine Graph + Vector data into rich LLM context

#### Tasks

**2.2.1 Context Fusion Implementation**

Location: `medusa-cli/src/medusa/context/fusion_engine.py`

```python
"""
Context Fusion Engine
Combines Neo4j graph data with vector DB semantic search
to build rich, intelligent context for LLM prompts
"""

from typing import Dict, Any, List, Optional
from medusa.world_model.client import WorldModelClient
from medusa.context.vector_store import VectorStore
import logging


class ContextFusionEngine:
    """
    Fuses multiple knowledge sources into unified LLM context

    Sources:
    1. Neo4j Graph - Current infrastructure state, relationships
    2. Vector DB - Semantic knowledge (MITRE, CVEs, tool docs)
    3. Operation History - Short-term memory of current session
    """

    def __init__(
        self,
        world_model: WorldModelClient,
        vector_store: VectorStore
    ):
        self.world_model = world_model
        self.vector_store = vector_store
        self.logger = logging.getLogger(__name__)

        # Short-term memory: current operation history
        self.operation_history: List[Dict[str, Any]] = []

    def build_context_for_reconnaissance(
        self,
        target: str,
        existing_findings: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """
        Build context for reconnaissance phase

        Returns:
            Rich context dict with graph state, MITRE techniques, tool suggestions
        """
        context = {
            "phase": "reconnaissance",
            "target": target
        }

        # 1. Graph: Check what we already know about this target
        existing_hosts = self.world_model.get_all_hosts(limit=10)
        context["known_infrastructure"] = {
            "host_count": len(existing_hosts),
            "hosts": existing_hosts[:5]  # Top 5
        }

        # 2. Vector: Relevant MITRE techniques for reconnaissance
        mitre_techniques = self.vector_store.search_mitre_techniques(
            query="network reconnaissance port scanning service discovery",
            n_results=5
        )
        context["recommended_techniques"] = mitre_techniques

        # 3. Vector: Tool usage examples
        tool_suggestions = self.vector_store.search_tool_usage(
            query="network port scanning service enumeration",
            n_results=3
        )
        context["tool_suggestions"] = tool_suggestions

        # 4. Operation history
        context["recent_actions"] = self.operation_history[-5:]

        self.logger.info(
            f"Context built: {len(mitre_techniques)} MITRE techniques, "
            f"{len(tool_suggestions)} tool suggestions"
        )

        return context

    def build_context_for_vulnerability_analysis(
        self,
        findings: List[Dict[str, Any]],
        target: str
    ) -> Dict[str, Any]:
        """Build context for vulnerability analysis phase"""
        context = {
            "phase": "vulnerability_analysis",
            "target": target,
            "findings_count": len(findings)
        }

        # 1. Graph: Get current vulnerabilities and relationships
        known_vulns = self.world_model.get_vulnerabilities()
        context["known_vulnerabilities"] = {
            "count": len(known_vulns),
            "high_severity": [v for v in known_vulns if v.get("severity") == "high"]
        }

        # 2. Extract unique services from findings
        services = set()
        for finding in findings:
            if finding.get("type") == "open_port":
                services.add(finding.get("service", "unknown"))

        # 3. Vector: Search for CVEs related to discovered services
        cve_context = []
        for service in list(services)[:5]:  # Limit to top 5 services
            cves = self.vector_store.search_cves(
                query=f"{service} vulnerability",
                n_results=3
            )
            cve_context.extend(cves)

        context["relevant_cves"] = cve_context

        # 4. Vector: Exploitation techniques
        exploit_techniques = self.vector_store.search_mitre_techniques(
            query="exploit vulnerability privilege escalation",
            n_results=5
        )
        context["exploitation_techniques"] = exploit_techniques

        return context

    def build_context_for_planning(
        self,
        all_findings: List[Dict[str, Any]],
        objectives: List[str]
    ) -> Dict[str, Any]:
        """
        Build comprehensive context for strategic planning

        This is for the Planning Agent - needs full picture
        """
        context = {
            "phase": "planning",
            "objectives": objectives,
            "total_findings": len(all_findings)
        }

        # 1. Graph: Complete attack surface
        attack_surface = self.world_model.get_graph_statistics()
        context["attack_surface"] = attack_surface

        # 2. Graph: Potential attack paths
        # (This would query for multi-hop paths in Neo4j)
        # Example: Domain -> Subdomain -> Host -> Vulnerability

        # 3. Vector: Historical successful attack chains
        similar_operations = self.vector_store.search_operation_history(
            query=f"Similar findings: {', '.join(objectives)}",
            n_results=3
        )
        context["similar_past_operations"] = similar_operations

        # 4. Vector: MITRE ATT&CK attack chain templates
        attack_chain_templates = self.vector_store.search_mitre_techniques(
            query="complete attack chain initial access persistence exfiltration",
            n_results=10
        )
        context["attack_chain_templates"] = attack_chain_templates

        # 5. Full operation history (for Planning Agent only)
        context["full_operation_history"] = self.operation_history

        return context

    def record_action(self, action: Dict[str, Any]):
        """Record an action to short-term memory"""
        self.operation_history.append({
            "timestamp": datetime.now().isoformat(),
            **action
        })

        # Keep only last 50 actions in memory
        if len(self.operation_history) > 50:
            self.operation_history = self.operation_history[-50:]

    def get_context_summary(self) -> str:
        """Generate human-readable context summary for LLM"""
        stats = self.vector_store.get_stats()
        graph_stats = self.world_model.get_graph_statistics()

        summary = f"""
# Current Knowledge Base Status

## Graph Database (Infrastructure State)
{json.dumps(graph_stats, indent=2)}

## Vector Database (Semantic Knowledge)
{json.dumps(stats, indent=2)}

## Operation History
- Actions recorded: {len(self.operation_history)}
- Recent actions: {len([a for a in self.operation_history if a.get('timestamp', '') > (datetime.now() - timedelta(minutes=10)).isoformat()])} in last 10 minutes
"""
        return summary
```

**2.2.2 Integration with LLM Client**

Update `medusa-cli/src/medusa/core/llm/client.py`:

```python
class LLMClient:
    def __init__(
        self,
        config: LLMConfig,
        provider: BaseLLMProvider,
        context_engine: Optional[ContextFusionEngine] = None  # NEW
    ):
        self.config = config
        self.provider = provider
        self.router = ModelRouter(config)
        self.context_engine = context_engine  # NEW

    async def generate_with_context(
        self,
        prompt: str,
        task_type: str,
        phase: str,
        additional_context: Optional[Dict[str, Any]] = None
    ) -> LLMResponse:
        """
        Generate with automatic context fusion

        Automatically injects relevant graph + vector context
        """
        if self.context_engine:
            # Build rich context based on phase
            if phase == "reconnaissance":
                context = self.context_engine.build_context_for_reconnaissance(
                    target=additional_context.get("target"),
                    existing_findings=additional_context.get("findings")
                )
            elif phase == "vulnerability_analysis":
                context = self.context_engine.build_context_for_vulnerability_analysis(
                    findings=additional_context.get("findings"),
                    target=additional_context.get("target")
                )
            elif phase == "planning":
                context = self.context_engine.build_context_for_planning(
                    all_findings=additional_context.get("findings"),
                    objectives=additional_context.get("objectives", [])
                )
            else:
                context = {}

            # Inject context into system prompt
            context_summary = json.dumps(context, indent=2)
            system_prompt = f"""
You are a penetration testing AI agent. You have access to the following context:

{context_summary}

Use this context to make informed decisions. Reference specific MITRE techniques,
CVEs, or tool commands from the context when applicable.
"""
        else:
            system_prompt = None

        return await self.generate_with_routing(
            prompt=prompt,
            task_type=task_type,
            system_prompt=system_prompt
        )
```

#### Deliverables

- âœ… ContextFusionEngine implementation
- âœ… Phase-specific context builders
- âœ… LLM client integration
- âœ… Context summarization
- âœ… Operation history tracking
- âœ… Integration tests

---

### Phase 2.3: Tool Documentation & CVE Indexing (Week 6-7)

**Objective**: Index tool docs and CVE database for vector search

#### Tasks

**2.3.1 Tool Documentation Scraper**

Location: `medusa-cli/scripts/index_tool_docs.py`

```python
"""
Extract and index tool documentation
Scrapes man pages, help outputs, and creates searchable index
"""

import subprocess
from medusa.context.vector_store import VectorStore


def extract_nmap_docs():
    """Extract Nmap command documentation"""
    docs = []

    # Common nmap scan types
    scan_types = [
        {
            "tool": "nmap",
            "command": "nmap -sV -p- <target>",
            "description": "Service version detection scan on all ports",
            "category": "reconnaissance",
            "examples": "nmap -sV -p- 192.168.1.1"
        },
        {
            "tool": "nmap",
            "command": "nmap -sC -sV <target>",
            "description": "Default script scan with version detection",
            "category": "reconnaissance",
            "examples": "nmap -sC -sV scanme.nmap.org"
        },
        # Add 20+ more common nmap commands
    ]

    return docs


def extract_sqlmap_docs():
    """Extract SQLMap documentation"""
    # Similar structure for SQLMap commands
    pass


def index_all_tools():
    """Index all tool documentation"""
    vector_store = VectorStore()

    all_docs = []
    all_docs.extend(extract_nmap_docs())
    all_docs.extend(extract_sqlmap_docs())
    # ... other tools

    vector_store.index_tool_documentation(all_docs)
    print(f"âœ… Indexed {len(all_docs)} tool documentation entries")


if __name__ == "__main__":
    index_all_tools()
```

**2.3.2 CVE Database Integration**

Location: `medusa-cli/src/medusa/context/cve_indexer.py`

```python
"""
CVE Database indexing
Downloads and indexes CVE data for semantic search
"""

import requests
from typing import List, Dict, Any


class CVEIndexer:
    """Index CVE database for vulnerability search"""

    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store

    def download_recent_cves(self, days: int = 365) -> List[Dict[str, Any]]:
        """
        Download recent CVEs from NVD

        Note: This would use the NVD API in production
        For now, we'll use a curated list of common CVEs
        """
        # In production: Use NVD API
        # https://services.nvd.nist.gov/rest/json/cves/2.0

        # For MVP: Curated list of high-impact CVEs
        common_cves = [
            {
                "cve_id": "CVE-2024-1234",
                "description": "SQL injection in healthcare web applications",
                "severity": "high",
                "cvss": 8.5,
                "affected_software": ["MySQL", "Web Applications"],
                "references": ["https://nvd.nist.gov/vuln/detail/CVE-2024-1234"]
            },
            # Add 100+ common CVEs
        ]

        return common_cves

    def index_cves(self, cves: List[Dict[str, Any]]):
        """Index CVEs into vector database"""
        collection = self.vector_store.collections["cve_database"]

        ids = [cve["cve_id"] for cve in cves]
        documents = [
            f"CVE {cve['cve_id']}: {cve['description']}\n"
            f"Severity: {cve['severity']}, CVSS: {cve['cvss']}\n"
            f"Affected: {', '.join(cve['affected_software'])}"
            for cve in cves
        ]
        metadatas = [
            {
                "cve_id": cve["cve_id"],
                "severity": cve["severity"],
                "cvss": cve["cvss"],
                "affected_software": ",".join(cve["affected_software"])
            }
            for cve in cves
        ]

        collection.add(ids=ids, documents=documents, metadatas=metadatas)
```

#### Deliverables

- âœ… Tool documentation indexer
- âœ… CVE database integration
- âœ… Nmap, SQLMap, Kerbrute docs indexed
- âœ… 100+ CVEs indexed
- âœ… Search validation tests

---

## ðŸ¤– Phase 3: Multi-Agent System (Weeks 8-12)

### Phase 3.1: Agent Architecture (Week 8)

**Objective**: Define base agent class and communication protocol

#### Tasks

**3.1.1 Base Agent Class**

Location: `medusa-cli/src/medusa/agents/base.py`

```python
"""
Base Agent Class
Defines the interface and common functionality for all MEDUSA agents
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from datetime import datetime
import logging


@dataclass
class AgentMessage:
    """Message passed between agents"""
    id: str
    sender: str  # Agent name
    receiver: str  # Agent name or "orchestrator" or "broadcast"
    message_type: str  # "task", "result", "query", "approval_request"
    payload: Dict[str, Any]
    priority: int = 1  # 1=low, 5=high
    timestamp: datetime = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()


@dataclass
class AgentTask:
    """Task assigned to an agent"""
    task_id: str
    task_type: str
    parameters: Dict[str, Any]
    priority: int = 1
    timeout_seconds: Optional[int] = None


@dataclass
class AgentResult:
    """Result returned by agent"""
    task_id: str
    status: str  # "success", "failure", "partial"
    data: Dict[str, Any]
    cost_usd: float = 0.0
    duration_seconds: float = 0.0
    error: Optional[str] = None


class BaseAgent(ABC):
    """
    Base class for all MEDUSA agents

    Each agent:
    - Specializes in a specific domain (recon, analysis, exploitation, etc.)
    - Has access to specific tools
    - Maintains its own short-term memory
    - Reports to the orchestrator
    - Can query other agents via orchestrator
    """

    def __init__(
        self,
        agent_id: str,
        llm_client: 'LLMClient',
        world_model: 'WorldModelClient',
        context_engine: 'ContextFusionEngine',
        config: Dict[str, Any] = None
    ):
        self.agent_id = agent_id
        self.llm_client = llm_client
        self.world_model = world_model
        self.context_engine = context_engine
        self.config = config or {}

        # Agent memory
        self.short_term_memory: List[Dict[str, Any]] = []
        self.current_task: Optional[AgentTask] = None

        # Metrics
        self.tasks_completed = 0
        self.tasks_failed = 0
        self.total_cost = 0.0

        self.logger = logging.getLogger(f"medusa.agent.{agent_id}")
        self.logger.info(f"Agent {agent_id} initialized")

    @abstractmethod
    async def process_task(self, task: AgentTask) -> AgentResult:
        """
        Process a task assigned by orchestrator

        Must be implemented by subclasses
        """
        pass

    @abstractmethod
    def get_capabilities(self) -> List[str]:
        """
        Return list of task types this agent can handle

        Example: ["port_scan", "service_enumeration", "subdomain_discovery"]
        """
        pass

    async def execute_task(self, task: AgentTask) -> AgentResult:
        """
        Execute task with error handling and metrics tracking

        This wraps the subclass's process_task implementation
        """
        import time
        start_time = time.time()

        self.current_task = task
        self.logger.info(f"Starting task: {task.task_id} ({task.task_type})")

        try:
            result = await self.process_task(task)
            result.duration_seconds = time.time() - start_time

            # Update metrics
            if result.status == "success":
                self.tasks_completed += 1
            else:
                self.tasks_failed += 1

            self.total_cost += result.cost_usd

            # Record to memory
            self.short_term_memory.append({
                "task_id": task.task_id,
                "task_type": task.task_type,
                "status": result.status,
                "timestamp": datetime.now().isoformat(),
                "cost": result.cost_usd
            })

            # Limit memory size
            if len(self.short_term_memory) > 100:
                self.short_term_memory = self.short_term_memory[-100:]

            self.logger.info(
                f"Task {task.task_id} completed: status={result.status}, "
                f"duration={result.duration_seconds:.1f}s, cost=${result.cost_usd:.4f}"
            )

            return result

        except Exception as e:
            self.logger.error(f"Task {task.task_id} failed: {e}", exc_info=True)
            self.tasks_failed += 1

            return AgentResult(
                task_id=task.task_id,
                status="failure",
                data={},
                error=str(e),
                duration_seconds=time.time() - start_time
            )
        finally:
            self.current_task = None

    def get_metrics(self) -> Dict[str, Any]:
        """Get agent performance metrics"""
        success_rate = (
            self.tasks_completed / (self.tasks_completed + self.tasks_failed)
            if (self.tasks_completed + self.tasks_failed) > 0
            else 0.0
        )

        return {
            "agent_id": self.agent_id,
            "tasks_completed": self.tasks_completed,
            "tasks_failed": self.tasks_failed,
            "success_rate": success_rate,
            "total_cost_usd": self.total_cost,
            "memory_size": len(self.short_term_memory)
        }

    def clear_memory(self):
        """Clear short-term memory"""
        self.short_term_memory = []
        self.logger.info("Short-term memory cleared")
```

**3.1.2 Message Bus**

Location: `medusa-cli/src/medusa/agents/message_bus.py`

```python
"""
Agent Message Bus
Handles inter-agent communication and message routing
"""

import asyncio
from typing import Dict, List, Optional
from collections import defaultdict
import logging


class MessageBus:
    """
    Central message bus for agent communication

    Supports:
    - Direct messaging (agent-to-agent)
    - Broadcast messages
    - Priority queues
    - Message filtering
    """

    def __init__(self):
        # Message queues per agent
        self.queues: Dict[str, asyncio.Queue] = defaultdict(asyncio.Queue)

        # Message history for debugging
        self.message_history: List[AgentMessage] = []

        self.logger = logging.getLogger("medusa.message_bus")

    async def send(self, message: AgentMessage):
        """Send message to target agent(s)"""
        self.message_history.append(message)

        if message.receiver == "broadcast":
            # Send to all agents
            for queue in self.queues.values():
                await queue.put(message)
            self.logger.debug(f"Broadcast message from {message.sender}")
        else:
            # Send to specific agent
            await self.queues[message.receiver].put(message)
            self.logger.debug(
                f"Message: {message.sender} -> {message.receiver} "
                f"(type={message.message_type})"
            )

    async def receive(self, agent_id: str, timeout: Optional[float] = None) -> Optional[AgentMessage]:
        """Receive next message for agent"""
        try:
            if timeout:
                message = await asyncio.wait_for(
                    self.queues[agent_id].get(),
                    timeout=timeout
                )
            else:
                message = await self.queues[agent_id].get()

            return message
        except asyncio.TimeoutError:
            return None

    def get_queue_size(self, agent_id: str) -> int:
        """Get number of pending messages for agent"""
        return self.queues[agent_id].qsize()

    def get_stats(self) -> Dict[str, Any]:
        """Get message bus statistics"""
        return {
            "total_messages": len(self.message_history),
            "queue_sizes": {
                agent_id: queue.qsize()
                for agent_id, queue in self.queues.items()
            }
        }
```

#### Deliverables

- âœ… BaseAgent abstract class
- âœ… AgentMessage, AgentTask, AgentResult data classes
- âœ… MessageBus implementation
- âœ… Agent metrics tracking
- âœ… Unit tests for base components

---

### Phase 3.2: Specialized Agents Implementation (Weeks 9-10)

**Objective**: Implement all 6 specialized agents

#### Agent 1: Reconnaissance Agent

Location: `medusa-cli/src/medusa/agents/recon_agent.py`

```python
"""
Reconnaissance Agent
Specializes in discovery and information gathering
"""

from medusa.agents.base import BaseAgent, AgentTask, AgentResult
from medusa.tools import NmapScanner, AmassScanner, HttpxScanner


class ReconnaissanceAgent(BaseAgent):
    """
    Reconnaissance specialist

    Capabilities:
    - Port scanning (Nmap)
    - Subdomain enumeration (Amass)
    - Web server probing (HTTPX)
    - Service fingerprinting

    Model: Claude 3.5 Haiku (fast, cost-effective)
    """

    def __init__(self, *args, **kwargs):
        super().__init__("recon_agent", *args, **kwargs)

        # Initialize tools
        self.nmap = NmapScanner(timeout=600)
        self.amass = AmassScanner(timeout=300, passive=True)
        self.httpx = HttpxScanner(timeout=120, threads=50)

    def get_capabilities(self) -> List[str]:
        return [
            "port_scan",
            "service_enumeration",
            "subdomain_discovery",
            "web_server_probing",
            "fingerprinting"
        ]

    async def process_task(self, task: AgentTask) -> AgentResult:
        """Process reconnaissance task"""
        task_type = task.task_type
        params = task.parameters

        if task_type == "port_scan":
            return await self._port_scan(params)
        elif task_type == "subdomain_discovery":
            return await self._subdomain_discovery(params)
        elif task_type == "web_server_probing":
            return await self._web_probing(params)
        else:
            raise ValueError(f"Unknown task type: {task_type}")

    async def _port_scan(self, params: Dict[str, Any]) -> AgentResult:
        """Execute port scan"""
        target = params["target"]
        ports = params.get("ports", "1-1000")

        # Get LLM recommendation for scan strategy
        context = self.context_engine.build_context_for_reconnaissance(target)

        llm_response = await self.llm_client.generate_with_context(
            prompt=f"Plan port scan strategy for target: {target}",
            task_type="plan_port_scan",
            phase="reconnaissance",
            additional_context={"target": target}
        )

        # Execute nmap scan
        nmap_result = await self.nmap.execute(
            target=target,
            ports=ports,
            scan_type="-sV"
        )

        # Store findings in Neo4j
        if nmap_result["success"]:
            for finding in nmap_result["findings"]:
                if finding["type"] == "open_port":
                    # Create Host and Port nodes
                    host = Host(
                        ip=finding.get("ip", target),
                        hostname=finding.get("hostname"),
                        os_name=finding.get("os"),
                        os_accuracy=finding.get("os_accuracy", 0),
                        discovered_at=datetime.now(),
                        last_seen=datetime.now()
                    )
                    self.world_model.create_host(host)

                    port = Port(
                        number=finding["port"],
                        protocol=finding.get("protocol", "tcp"),
                        host_id=finding.get("ip", target),
                        state="open",
                        service=finding.get("service"),
                        product=finding.get("product"),
                        version=finding.get("version"),
                        discovered_at=datetime.now()
                    )
                    self.world_model.create_port(port, finding.get("ip", target))

        # Record action to context engine
        self.context_engine.record_action({
            "agent": self.agent_id,
            "action": "port_scan",
            "target": target,
            "findings_count": nmap_result["findings_count"]
        })

        return AgentResult(
            task_id=task.task_id,
            status="success" if nmap_result["success"] else "failure",
            data={
                "findings": nmap_result["findings"],
                "findings_count": nmap_result["findings_count"],
                "scan_duration": nmap_result["duration_seconds"]
            },
            cost_usd=llm_response.metadata.get("cost_usd", 0.0)
        )

    async def _subdomain_discovery(self, params: Dict[str, Any]) -> AgentResult:
        """Execute subdomain enumeration"""
        # Similar implementation for Amass
        pass

    async def _web_probing(self, params: Dict[str, Any]) -> AgentResult:
        """Execute web server probing"""
        # Similar implementation for HTTPX
        pass
```

#### Agent 2: Vulnerability Analysis Agent

Location: `medusa-cli/src/medusa/agents/vuln_analysis_agent.py`

```python
"""
Vulnerability Analysis Agent
Identifies and assesses security weaknesses
"""

from medusa.agents.base import BaseAgent, AgentTask, AgentResult
from medusa.tools import SQLMapScanner


class VulnerabilityAnalysisAgent(BaseAgent):
    """
    Vulnerability analysis specialist

    Capabilities:
    - SQL injection detection (SQLMap)
    - Misconfiguration analysis
    - CVE matching
    - Vulnerability prioritization

    Model: Claude 3.5 Haiku
    """

    def __init__(self, *args, **kwargs):
        super().__init__("vuln_analysis_agent", *args, **kwargs)
        self.sqlmap = SQLMapScanner(timeout=600)

    def get_capabilities(self) -> List[str]:
        return [
            "sql_injection_test",
            "misconfiguration_analysis",
            "cve_matching",
            "vulnerability_prioritization"
        ]

    async def process_task(self, task: AgentTask) -> AgentResult:
        """Process vulnerability analysis task"""
        task_type = task.task_type
        params = task.parameters

        if task_type == "sql_injection_test":
            return await self._test_sql_injection(params)
        elif task_type == "cve_matching":
            return await self._match_cves(params)
        elif task_type == "vulnerability_prioritization":
            return await self._prioritize_vulnerabilities(params)
        else:
            raise ValueError(f"Unknown task type: {task_type}")

    async def _test_sql_injection(self, params: Dict[str, Any]) -> AgentResult:
        """Test for SQL injection vulnerabilities"""
        url = params["url"]

        # Get context from fusion engine
        context = self.context_engine.build_context_for_vulnerability_analysis(
            findings=params.get("previous_findings", []),
            target=url
        )

        # LLM decides test strategy
        llm_response = await self.llm_client.generate_with_context(
            prompt=f"Plan SQL injection testing strategy for: {url}",
            task_type="plan_sql_injection_test",
            phase="vulnerability_analysis",
            additional_context={"url": url, "context": context}
        )

        # Execute SQLMap
        sqlmap_result = await self.sqlmap.test_injection(
            url=url,
            method=params.get("method", "GET"),
            level=params.get("level", 1),
            risk=params.get("risk", 1)
        )

        # Store vulnerabilities in Neo4j
        if sqlmap_result["metadata"].get("vulnerable"):
            for vuln in sqlmap_result["findings"]:
                vulnerability = Vulnerability(
                    id=f"sqli_{url}_{datetime.now().timestamp()}",
                    type="sql_injection",
                    parameter=vuln.get("parameter"),
                    location=url,
                    dbms=vuln.get("dbms"),
                    severity="high",
                    exploited=False,
                    discovered_at=datetime.now()
                )
                self.world_model.create_vulnerability(
                    vulnerability,
                    target_url=url
                )

        return AgentResult(
            task_id=task.task_id,
            status="success",
            data={
                "vulnerable": sqlmap_result["metadata"].get("vulnerable"),
                "findings": sqlmap_result["findings"]
            },
            cost_usd=llm_response.metadata.get("cost_usd", 0.0)
        )

    async def _match_cves(self, params: Dict[str, Any]) -> AgentResult:
        """Match discovered services to known CVEs"""
        services = params["services"]

        # Use vector store to search for relevant CVEs
        all_cves = []
        for service in services:
            cves = self.context_engine.vector_store.search_cves(
                query=f"{service['name']} {service.get('version', '')} vulnerability",
                n_results=5
            )
            all_cves.extend(cves)

        # LLM assesses relevance and priority
        llm_response = await self.llm_client.generate_with_context(
            prompt=f"Assess CVE relevance and prioritize: {json.dumps(all_cves)}",
            task_type="prioritize_cves",
            phase="vulnerability_analysis",
            additional_context={"services": services, "cves": all_cves}
        )

        return AgentResult(
            task_id=task.task_id,
            status="success",
            data={"cves": all_cves, "prioritization": llm_response.content},
            cost_usd=llm_response.metadata.get("cost_usd", 0.0)
        )
```

#### Agent 3: Exploitation Agent

Location: `medusa-cli/src/medusa/agents/exploit_agent.py`

```python
"""
Exploitation Agent
Executes attacks on identified vulnerabilities
âš ï¸ REQUIRES APPROVAL GATES âš ï¸
"""

from medusa.agents.base import BaseAgent, AgentTask, AgentResult
from medusa.approval import ApprovalGate, RiskLevel


class ExploitationAgent(BaseAgent):
    """
    Exploitation specialist

    Capabilities:
    - Exploit execution
    - Credential testing
    - Privilege escalation

    Model: Claude 3.5 Haiku

    âš ï¸ ALL ACTIONS REQUIRE APPROVAL âš ï¸
    """

    def __init__(self, approval_gate: ApprovalGate, *args, **kwargs):
        super().__init__("exploit_agent", *args, **kwargs)
        self.approval_gate = approval_gate

        # Exploitation is HIGH RISK
        self.default_risk_level = RiskLevel.HIGH

    def get_capabilities(self) -> List[str]:
        return [
            "exploit_sql_injection",
            "test_credentials",
            "privilege_escalation"
        ]

    async def process_task(self, task: AgentTask) -> AgentResult:
        """Process exploitation task"""

        # âš ï¸ APPROVAL REQUIRED
        approval_granted = await self.approval_gate.request_approval(
            action=f"Exploit: {task.task_type}",
            risk_level=self.default_risk_level,
            details=task.parameters
        )

        if not approval_granted:
            return AgentResult(
                task_id=task.task_id,
                status="failure",
                data={},
                error="Approval denied by user"
            )

        # Execute exploitation
        if task.task_type == "exploit_sql_injection":
            return await self._exploit_sql_injection(task.parameters)
        else:
            raise ValueError(f"Unknown task type: {task.task_type}")

    async def _exploit_sql_injection(self, params: Dict[str, Any]) -> AgentResult:
        """Exploit SQL injection vulnerability"""
        # Implementation with safety checks
        pass
```

#### Agent 4: Strategic Planning Agent

Location: `medusa-cli/src/medusa/agents/planning_agent.py`

```python
"""
Strategic Planning Agent
Generates comprehensive attack strategies
"""

from medusa.agents.base import BaseAgent, AgentTask, AgentResult


class StrategyPlanningAgent(BaseAgent):
    """
    Strategic planning specialist

    Capabilities:
    - Attack chain planning
    - Multi-step strategy generation
    - Risk-reward analysis
    - Tool selection optimization

    Model: Claude 3.5 Sonnet (requires deep reasoning)
    """

    def __init__(self, *args, **kwargs):
        super().__init__("planning_agent", *args, **kwargs)

        # Override model to use Sonnet (smarter model)
        if hasattr(self.llm_client.provider, 'model'):
            self.llm_client.provider.model = self.config.get(
                "smart_model",
                "anthropic.claude-3-5-sonnet-20241022-v2:0"
            )

    def get_capabilities(self) -> List[str]:
        return [
            "generate_attack_plan",
            "optimize_attack_chain",
            "assess_risk_reward",
            "suggest_tool_combinations"
        ]

    async def process_task(self, task: AgentTask) -> AgentResult:
        """Process planning task"""

        if task.task_type == "generate_attack_plan":
            return await self._generate_attack_plan(task.parameters)
        else:
            raise ValueError(f"Unknown task type: {task.task_type}")

    async def _generate_attack_plan(self, params: Dict[str, Any]) -> AgentResult:
        """Generate comprehensive attack strategy"""

        # Build rich context (full knowledge base)
        context = self.context_engine.build_context_for_planning(
            all_findings=params["findings"],
            objectives=params["objectives"]
        )

        # Use Sonnet for deep reasoning
        llm_response = await self.llm_client.generate_with_context(
            prompt=f"""
Generate a comprehensive attack plan based on all findings.

Objectives: {params['objectives']}

Your plan should include:
1. Attack chain sequence
2. MITRE ATT&CK technique mapping
3. Tool recommendations
4. Risk assessment per step
5. Success probability estimation
6. Contingency plans

Output as structured JSON.
""",
            task_type="generate_attack_plan",
            phase="planning",
            additional_context=context,
            force_json=True
        )

        attack_plan = json.loads(llm_response.content)

        return AgentResult(
            task_id=task.task_id,
            status="success",
            data={"attack_plan": attack_plan},
            cost_usd=llm_response.metadata.get("cost_usd", 0.0)
        )
```

#### Agents 5 & 6: Reporting Agent, Orchestrator Agent

*(Similar structure - detailed implementation available upon request)*

#### Deliverables

- âœ… All 6 agents implemented
- âœ… Tool integrations per agent
- âœ… Approval gates for exploitation
- âœ… Context fusion integration
- âœ… Cost tracking per agent
- âœ… Unit tests for each agent

---

### Phase 3.3: Orchestrator Implementation (Week 11)

**Objective**: Implement supervisor orchestrator that coordinates all agents

Location: `medusa-cli/src/medusa/agents/orchestrator.py`

```python
"""
Orchestrator Agent (Supervisor)
Coordinates all specialist agents and manages operation flow
"""

from medusa.agents.base import BaseAgent, AgentTask, AgentResult, AgentMessage
from medusa.agents.message_bus import MessageBus
from typing import Dict, List


class OrchestratorAgent:
    """
    Master orchestrator for multi-agent system

    Responsibilities:
    - Receives user goals
    - Decomposes into tasks
    - Delegates to specialist agents
    - Manages approval gates
    - Tracks overall progress
    - Handles errors and retries

    Model: Claude 3.5 Sonnet (strategic decision-making)
    """

    def __init__(
        self,
        llm_client: 'LLMClient',
        world_model: 'WorldModelClient',
        context_engine: 'ContextFusionEngine',
        message_bus: MessageBus,
        agents: Dict[str, BaseAgent],
        approval_gate: 'ApprovalGate'
    ):
        self.llm_client = llm_client
        self.world_model = world_model
        self.context_engine = context_engine
        self.message_bus = message_bus
        self.agents = agents
        self.approval_gate = approval_gate

        # Operation state
        self.current_operation: Optional[str] = None
        self.operation_goals: List[str] = []
        self.operation_phase: str = "idle"

        self.logger = logging.getLogger("medusa.orchestrator")

    async def start_operation(
        self,
        target: str,
        objectives: List[str],
        mode: str = "autonomous"
    ):
        """
        Start a new penetration testing operation

        Args:
            target: Target URL or IP
            objectives: List of objectives (e.g., ["assess_security", "test_sql_injection"])
            mode: Operation mode (autonomous, interactive, observe)
        """
        self.current_operation = f"op_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.operation_goals = objectives
        self.operation_phase = "reconnaissance"

        self.logger.info(
            f"Starting operation {self.current_operation}: "
            f"target={target}, objectives={objectives}"
        )

        # Phase 1: Reconnaissance
        recon_tasks = await self._plan_reconnaissance(target)
        recon_results = await self._execute_tasks(recon_tasks)

        # Phase 2: Vulnerability Analysis
        analysis_tasks = await self._plan_vulnerability_analysis(target, recon_results)
        analysis_results = await self._execute_tasks(analysis_tasks)

        # Phase 3: Strategic Planning
        attack_plan = await self._generate_attack_plan(
            target,
            recon_results + analysis_results,
            objectives
        )

        # Phase 4: Exploitation (if approved)
        if mode == "autonomous":
            exploit_results = await self._execute_attack_plan(attack_plan)

        # Phase 5: Reporting
        report = await self._generate_report()

        return report

    async def _plan_reconnaissance(self, target: str) -> List[AgentTask]:
        """Use LLM to plan reconnaissance phase"""

        # Get LLM recommendation
        llm_response = await self.llm_client.generate_with_context(
            prompt=f"""
Plan the reconnaissance phase for target: {target}

Generate a list of tasks to assign to the Reconnaissance Agent.
Each task should have: task_id, task_type, parameters, priority

Available task types:
- port_scan
- subdomain_discovery
- web_server_probing

Output as JSON array.
""",
            task_type="plan_reconnaissance",
            phase="planning",
            additional_context={"target": target}
        )

        tasks_data = json.loads(llm_response.content)

        tasks = [
            AgentTask(
                task_id=t["task_id"],
                task_type=t["task_type"],
                parameters=t["parameters"],
                priority=t.get("priority", 1)
            )
            for t in tasks_data
        ]

        return tasks

    async def _execute_tasks(self, tasks: List[AgentTask]) -> List[AgentResult]:
        """Execute tasks by delegating to appropriate agents"""
        results = []

        for task in tasks:
            # Find agent with capability
            agent = self._find_agent_for_task(task.task_type)

            if not agent:
                self.logger.error(f"No agent found for task type: {task.task_type}")
                continue

            # Execute task
            self.logger.info(f"Delegating {task.task_type} to {agent.agent_id}")
            result = await agent.execute_task(task)
            results.append(result)

            # Update context engine
            self.context_engine.record_action({
                "agent": agent.agent_id,
                "task": task.task_type,
                "status": result.status
            })

        return results

    def _find_agent_for_task(self, task_type: str) -> Optional[BaseAgent]:
        """Find agent that can handle this task type"""
        for agent in self.agents.values():
            if task_type in agent.get_capabilities():
                return agent
        return None

    async def _generate_attack_plan(
        self,
        target: str,
        all_results: List[AgentResult],
        objectives: List[str]
    ) -> Dict[str, Any]:
        """Delegate to Planning Agent for strategic planning"""

        planning_agent = self.agents["planning_agent"]

        task = AgentTask(
            task_id=f"plan_{self.current_operation}",
            task_type="generate_attack_plan",
            parameters={
                "target": target,
                "findings": [r.data for r in all_results],
                "objectives": objectives
            }
        )

        result = await planning_agent.execute_task(task)
        return result.data.get("attack_plan", {})

    async def get_operation_status(self) -> Dict[str, Any]:
        """Get current operation status"""
        agent_metrics = {
            agent_id: agent.get_metrics()
            for agent_id, agent in self.agents.items()
        }

        return {
            "operation_id": self.current_operation,
            "phase": self.operation_phase,
            "objectives": self.operation_goals,
            "agents": agent_metrics,
            "graph_stats": self.world_model.get_graph_statistics(),
            "vector_stats": self.context_engine.vector_store.get_stats()
        }
```

#### Deliverables

- âœ… Orchestrator implementation
- âœ… Task planning with LLM
- âœ… Agent delegation logic
- âœ… Phase management
- âœ… Error handling and retries
- âœ… Operation status tracking

---

### Phase 3.4: Integration & Testing (Week 12)

**Objective**: End-to-end integration and validation

#### Tasks

1. **Integration Testing**
   - Full operation flow test (recon â†’ analysis â†’ planning â†’ exploit â†’ report)
   - Multi-agent coordination validation
   - Message bus stress testing
   - Cost tracking validation

2. **CLI Integration**
   - Update existing CLI commands to use multi-agent system
   - Backward compatibility with old single-agent modes
   - New commands: `medusa multi-agent`, `medusa agent-status`

3. **Documentation**
   - Agent architecture diagrams
   - API documentation for each agent
   - User guide for multi-agent mode
   - Cost optimization guide

4. **Performance Optimization**
   - Parallel task execution
   - Context caching
   - LLM response caching

#### Deliverables

- âœ… Full integration test suite passing
- âœ… CLI updated with multi-agent support
- âœ… Performance benchmarks
- âœ… Complete documentation
- âœ… Migration guide from single-agent

---

## ðŸ“Š Success Metrics

### Phase 1: AWS Bedrock
- âœ… Bedrock provider health check passes
- âœ… Cost tracking accurate within 1%
- âœ… Smart/fast model routing reduces costs by 40%+
- âœ… Fallback to local working seamlessly

### Phase 2: Context Fusion
- âœ… 200+ MITRE techniques indexed
- âœ… 100+ CVEs indexed
- âœ… Tool docs for 6 tools indexed
- âœ… Vector search returns relevant results 90%+ of time
- âœ… Context injection improves LLM relevance (user validation)

### Phase 3: Multi-Agent
- âœ… All 6 agents operational
- âœ… Orchestrator successfully coordinates complex operations
- âœ… Agent success rate > 85%
- âœ… Cost per operation < $0.50 (with Haiku routing)
- âœ… End-to-end operation completes in < 10 minutes

---

## ðŸ”’ Security Considerations

1. **Approval Gates**
   - Exploitation agent ALWAYS requires approval
   - High-risk actions flagged prominently
   - Audit log of all approvals

2. **Credential Security**
   - AWS credentials never logged
   - Bedrock API keys encrypted at rest
   - Neo4j credentials in secure config

3. **Rate Limiting**
   - Bedrock rate limit handling
   - Exponential backoff on failures
   - Cost limit per operation ($5 default max)

4. **Data Privacy**
   - Vector DB does not store sensitive findings
   - Graph DB access controlled
   - Operation history sanitized before indexing

---

## ðŸ’° Cost Estimates

### Development Costs (AWS Bedrock during implementation)

- **Phase 1 Testing**: ~$50 (provider testing, integration tests)
- **Phase 2 Testing**: ~$30 (embedding generation, vector search tests)
- **Phase 3 Testing**: ~$100 (multi-agent coordination tests)
- **Total Development**: ~$180

### Operational Costs (per operation)

**With Smart Routing (Recommended)**:
- Reconnaissance: 5K tokens @ Haiku = $0.004
- Vulnerability Analysis: 8K tokens @ Haiku = $0.006
- Planning: 15K tokens @ Sonnet = $0.045
- Reporting: 10K tokens @ Haiku = $0.008
- **Total per operation**: ~$0.06-0.15

**Without Routing (all Sonnet)**:
- **Total per operation**: ~$0.50-0.80

**Cost Savings with Smart Routing**: 70-80%

---

## ðŸ“… Timeline Summary

| Phase | Duration | Deliverables |
|-------|----------|--------------|
| **Phase 1.1** | Week 1 | Bedrock provider, factory integration |
| **Phase 1.2** | Week 2 | Model routing, cost optimization |
| **Phase 1.3** | Week 3 | Cost tracking, reporting |
| **Phase 2.1** | Week 4 | Vector DB setup, MITRE indexing |
| **Phase 2.2** | Week 5 | Context Fusion Engine |
| **Phase 2.3** | Week 6-7 | Tool docs, CVE indexing |
| **Phase 3.1** | Week 8 | Base agent, message bus |
| **Phase 3.2** | Week 9-10 | All 6 specialized agents |
| **Phase 3.3** | Week 11 | Orchestrator |
| **Phase 3.4** | Week 12 | Integration, testing, docs |

**Total**: 12 weeks (3 months)

---

## ðŸš¦ Implementation Order

### Week-by-Week Breakdown

**Weeks 1-3: AWS Foundation**
- Get Bedrock working end-to-end
- Validate cost tracking
- Test model routing

**Weeks 4-7: Knowledge Infrastructure**
- Vector DB operational
- MITRE, CVEs, tool docs indexed
- Context fusion delivering value

**Weeks 8-12: Agent System**
- Agents implemented sequentially
- Orchestrator ties everything together
- Full system validation

### Critical Path Dependencies

1. Bedrock â†’ Cost Tracking â†’ Model Routing
2. Vector DB â†’ MITRE Indexing â†’ Context Fusion
3. Base Agent â†’ Specialized Agents â†’ Orchestrator
4. Context Fusion + Agents â†’ Full System

---

## ðŸŽ“ Learning Outcomes

By implementing this architecture, you will have:

1. **Production-grade multi-agent system** using modern LLM orchestration
2. **Hybrid database architecture** (Graph + Vector) for intelligent context
3. **Cost-optimized LLM usage** with smart model routing
4. **Enterprise cloud integration** with AWS Bedrock
5. **Real-world security platform** suitable for portfolio/publication

---

## ðŸ“š References

- [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [Neo4j Graph Database](https://neo4j.com/docs/)
- [MITRE ATT&CK Framework](https://attack.mitre.org/)
- [LangChain Multi-Agent Systems](https://python.langchain.com/docs/modules/agents/)

---

## ðŸ“ Next Steps

1. **Review this plan** with advisor/team
2. **Set up AWS Bedrock account** and get API credentials
3. **Create development branch**: `git checkout -b feature/multi-agent-evolution`
4. **Start Phase 1.1**: Bedrock provider implementation
5. **Track progress** using GitHub Projects or similar

---

**Document Version**: 1.0
**Last Updated**: 2025-11-12
**Author**: MEDUSA Architecture Team
**Status**: Ready for Implementation

---

**Navigation**: [Home](../../README.md) â†’ [Docs](../INDEX.md) â†’ [Architecture](README.md) â†’ Multi-Agent Evolution Plan
